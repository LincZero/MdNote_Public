# 探索IA处理器上最艰巨的任务

在通用处理器上处理包的最大挑战是什么？为什么以往通用处理器很少在数据面中扮演重要的角色？如果我们带着这些问题来看数据面上的负载，就会有一个比较直观的理解。这里拿40Gbit/s的速率作为考察包转发能力的样本。如图1-8所示，曲线为不同大小的包的最大理论转发能力。

线速情况下的报文的**指令成本**

![img](04.%20探索IA处理器上最艰巨的任务.assets/Image00044.jpg)

计算

> 分别截取64B和1024B数据包长，图1-8所示的线速情况下的报文的指令成本能明显地说明不同报文大小给系统带来的巨大差异。
>
> - **包到达间隔**：就如我们在包转发率那一节中理解的，对于越小的包，相邻包到达的时间间隔就越小。64B 和 1024B 分别为*16.8ns 和 208.8ns*。
> - **允许消耗**：假设CPU的主频率是2GHz，要达到理论最大的转发能力，对于64B和1024B软件分别允许消耗 *33 和 417 个时钟周期*。
> - **实际消耗 - 访存部分**：在*存储转发（store-forward）模型*下，报文收发以及查表都需要访存。那就对比一下访存的时钟周期。
>   - 一次LLC (最后一级缓存) 命中需要大约 *40个时钟周期*
>   - 如果LLC未命中，一次内存的读就需要 *70ns*
> - **差距**：换句话说，对于64B大小的包，即使每次都能命中LLC，*40个时钟周期依然离33有距离*。显然，小包处理时延对于通用CPU系统架构的挑战是巨大的。

解决

> 那是否说明IA就完全不适合高性能的网络负载呢？答案是否定的。证明这样的结论我们从两个方面入手，一个是IA平台实际能提供的最大能力，另一个是这个能力是否足以应对一定领域的高性能网络负载。
>
> DPDK的出现充分释放了IA平台对包处理的吞吐能力。我们知道，随着吞吐率的上升，中断触发的开销是不能忍受的，DPDK通过一系列软件优化方法（*大页利用，cache对齐，线程绑定，NUMA感知，内存通道交叉访问，无锁化数据结构，预取，SIMD指令利用等*）利用IA平台硬件特性，提供完整的底层开发支持库。使得单核三层转发可以轻松地突破小包30Mpps，随着CPU封装的核数越来越多，支持的PCIe通道数越来越多，整系统的三层转发吞吐在2路CPU的Xeon E5-2658v3上可以达到300Mpps。这已经是一个相当可观的转发吞吐能力了。
>
> 虽然这个能力不足以覆盖网络中所有端到端的设备场景，但无论在核心网接入侧，还是在数据中心网络中，都已经可以覆盖相当多的场景。
>
> 随着数据面可软化的发生，数据面的设计、开发、验证乃至部署会发生一系列的变化。
>
> - 首先，可以采用通用服务器平台，降低专门硬件设计成本；
> - 其次，基于C语言的开发，就程序员数量以及整个生态都要比专门硬件开发更丰富；
> - 另外，灵活可编程的数据面部署也给网络功能虚拟化（NFV）带来了可能，更会进一步推进软件定义网络（SDN）的全面展开。







