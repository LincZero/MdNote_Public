# eBPF

# 目录

# 程序类型

在撰写本文时，有 18 种不同的 BPF 程序类型可用，其中**两种主要的网络类型**将在下面的小节中进一步解释，即 XDP BPF 程序和 tc BPF 程序。 
LLVM、iproute2 或其他工具的两种程序类型的广泛使用示例遍布整个工具链部分，此处不予介绍。相反，本节重点介绍它们的架构、概念和用例。

## XDP

XDP 代表 eXpress Data Path，提供了 BPF 框架，可在 Linux 内核中实现高性能可编程数据包处理。它在软件中尽早运行 BPF 程序，即在网络驱动程序收到数据包时。

此时，在快速路径中，驱动程序只是从其接收环中拾取数据包，而没有执行任何昂贵的操作，例如分配 `skb` 来将数据包进一步推入网络堆栈，而无需将数据包推入网络堆栈。因此，XDP BPF 程序在可供 CPU 处理时最早执行。

XDP 与 Linux 内核及其基础设施协同工作，这意味着内核不会像在仅在用户空间运行的各种网络框架中那样被绕过。将数据包保留在内核空间有几个主要优点：

- XDP 能够在 BPF 帮助程序调用自身中重用所有上游开发的内核网络驱动程序、用户空间工具，甚至其他可用的内核基础设施，例如路由表、套接字等。
- XDP 驻留在内核空间中，与访问硬件的内核其余部分具有相同的安全模型。
- 不需要跨越内核/用户空间边界，因为处理后的数据包已经驻留在内核中，因此可以灵活地将数据包转发到其他内核实体，例如容器使用的命名空间或内核的网络堆栈本身。这在崩溃和幽灵时期尤其重要。
- 将数据包从 XDP 发送到内核强大、广泛使用且高效的 TCP/IP 堆栈是完全可能的，允许完全重用，并且不需要像用户空间框架那样维护单独的 TCP/IP 堆栈。
- BPF 的使用允许完全可编程性，保持稳定的 ABI，并具有与内核的系统调用 ABI 相同的“永不破坏用户空间”保证，并且与模块相比，它还提供了安全措施，这要归功于 BPF 验证器，可确保内核运行的稳定性。
- XDP 允许在运行时自动交换程序，而无需任何网络流量中断，甚至内核/系统重新启动。
- XDP 允许灵活地构建集成到内核中的工作负载。例如，它可以在“忙轮询”或“中断驱动”模式下运行。不需要明确地将 CPU 专用于 XDP。没有特殊的硬件要求，也不依赖大页。
- XDP 不需要任何第三方内核模块或许可。它是一个长期的架构解决方案，是Linux内核的核心部分，由内核社区开发。
- XDP 已在运行相当于 4.8 或更高版本内核的主要发行版中启用并随处提供，并支持大多数主要 10G 或更高版本的网络驱动程序。

作为在驱动程序中运行 BPF 的框架，XDP 还确保数据包线性布局并适合 BPF 程序可读和可写的单个 DMA 页面。 XDP 还确保程序可以使用 256 字节的额外空间，以在 `bpf_xdp_adjust_head()` BPF 帮助程序的帮助下实现自定义封装标头，或通过 `bpf_xdp_adjust_meta()` 在数据包前面添加自定义元数据。

该框架包含在下面的部分中进一步描述的 XDP 操作代码，BPF 程序可以返回这些代码，以指示驱动程序如何继续处理数据包，并且它可以自动替换在 XDP 层运行的 BPF 程序。  XDP 专为高性能而设计。 BPF  允许通过“直接数据包访问”来访问数据包数据，这意味着程序将数据指针直接保存在寄存器中，将内容加载到寄存器中，然后分别从寄存器写入数据包。

作为 BPF 上下文传递给 BPF 程序的 XDP 中的数据包表示形式如下所示：

```c
struct xdp_buff {
    void *data;
    void *data_end;
    void *data_meta;
    void *data_hard_start;
    struct xdp_rxq_info *rxq;
};
```

`data` 指向页面中数据包数据的开始，顾名思义， `data_end` 指向数据包数据的结束。由于 XDP 允许净空， `data_hard_start` 指向页面中最大可能的净空起始位置，这意味着，当应封装数据包时， `data` 会移近 `data_hard_start` 。相同的 BPF 辅助函数还允许解封装，在这种情况下 `data` 会远离 `data_hard_start` 。

`data_meta` 最初指向与 `data` 相同的位置，但 `bpf_xdp_adjust_meta()` 也能够将指针移向 `data_hard_start` 以提供空间用于自定义元数据，该元数据对于普通内核网络堆栈不可见，但可以由 tc BPF 程序读取，因为它是从 XDP 传输到 `skb` 的。反之亦然，它可以通过相同的 BPF 辅助函数通过再次将 `data_meta` 移离 `data_hard_start` 来删除或减小自定义元数据的大小。 `data_meta` 也可以单独用于在尾部调用之间传递状态，类似于 tc BPF 程序中可访问的 `skb->cb[]` 控制块情况。

这给出了 `struct xdp_buff` 数据包指针分别不变的以下关系： `data_hard_start` <= `data_meta` <= `data` < `data_end`

`rxq` 字段指向每个接收队列的一些附加元数据，这些元数据在环设置时（而不是在 XDP 运行时）填充：

```c
struct xdp_rxq_info {
    struct net_device *dev;
    u32 queue_index;
    u32 reg_state;
} ____cacheline_aligned;
```

BPF 程序可以检索 `queue_index` 以及来自网络设备本身的附加数据，例如 `ifindex` 等。

### BPF 程序返回码

运行 XDP BPF 程序后，程序会返回一个判决，以告诉驱动程序下一步如何处理数据包。在 `linux/bpf.h` 系统头文件中枚举了所有可用的返回判决：

```c
enum xdp_action {
    XDP_ABORTED = 0,
    XDP_DROP,
    XDP_PASS,
    XDP_TX,
    XDP_REDIRECT,
};
```

 `XDP_DROP` 顾名思义，将在驱动程序级别丢弃数据包，而不会浪费任何进一步的资源。这对于实施 DDoS 缓解机制或一般防火墙的 BPF 程序特别有用。 `XDP_PASS` 返回码意味着数据包被允许传递到内核的网络堆栈。这意味着，当前处理此数据包的 CPU 现在分配一个 `skb` ，填充它，并将其向前传递到 GRO 引擎。这相当于没有 XDP 时的默认数据包处理行为。通过 `XDP_TX` ，BPF 程序可以有效地选择将网络数据包从刚刚到达的同一 NIC 传输出去。当很少有节点实现时，这通常很有用，例如，在集群中实现防火墙和后续负载平衡，从而充当发夹式负载平衡器，在将传入数据包重写到 XDP BPF 中后将其推回交换机。 `XDP_REDIRECT` 与 `XDP_TX` 类似，它能够传输 XDP 数据包，但通过另一个 NIC。 `XDP_REDIRECT` 情况的另一个选项是重定向到 BPF cpumap，这意味着，在 NIC 接收队列上提供 XDP 服务的 CPU 可以继续这样做，并将用于处理上层内核堆栈的数据包推送到远程 CPU 。这与 `XDP_PASS` 类似，但 XDP BPF 程序能够继续为传入的高负载提供服务，而不是暂时在当前数据包上花费工作以推送到上层。最后但并非最不重要的一点是， `XDP_ABORTED` 用于表示程序中的异常状态，并且与 `XDP_DROP` 具有相同的行为，只是 `XDP_ABORTED` 传递 `trace_xdp_exception` 

### XDP 的用例

本小节介绍了 XDP 的一些主要用例。该列表并不详尽，考虑到 XDP 和 BPF 所实现的可编程性和效率，它可以轻松地进行调整以解决非常具体的用例。

- DDoS 缓解、防火墙

  XDP BPF 的基本功能之一是告诉驱动程序在早期阶段用 `XDP_DROP` 丢弃数据包，从而允许执行任何类型的高效网络策略，并且每个数据包的成本极低。这在需要应对任何类型的 DDoS 攻击的情况下是理想的，而且更通用地允许在 BPF 中几乎没有开销的情况下实现任何类型的防火墙策略，例如在任何一种情况下，作为独立设备（例如通过 `XDP_TX` 清理“干净”流量）或广泛部署在保护终端主机本身的节点上（通过 `XDP_PASS` 或 cpumap `XDP_REDIRECT` 以保证良好的交通）。卸载的 XDP 甚至更进一步，将本来就很小的每数据包成本完全转移到 NIC 中并以线速进行处理。

- 转发和负载均衡

  XDP 的另一个主要用例是通过 `XDP_TX` 或 `XDP_REDIRECT` 操作进行数据包转发和负载平衡。数据包可以被 XDP 层中运行的 BPF 程序任意破坏，甚至可以使用 BPF 辅助函数来增加或减少数据包的净空，以便在再次发送数据包之前任意封装或解封装数据包。通过 `XDP_TX` 可以实现发夹式负载均衡器，将数据包从其最初到达的同一网络设备中推出，或者通过 `XDP_REDIRECT` 操作将其转发到另一个 NIC 进行传输。后一个返回代码还可以与 BPF 的 cpumap 结合使用，以对数据包进行负载平衡，以传递本地堆栈，但在远程非 XDP 处理 CPU 上。

- 叠前过滤/处理

  除了策略执行之外，XDP 还可以在 `XDP_DROP` case 的帮助下用于强化内核的网络堆栈，这意味着它可以在网络堆栈发现之前尽早丢弃本地节点的不相关数据包他们例如鉴于我们知道节点仅提供 TCP 流量，任何 UDP、SCTP 或其他 L4 流量都可以立即丢弃。这样做的优点是，数据包不需要遍历各种实体，例如 GRO  引擎、内核的流解析器等，然后才能确定丢弃它们，从而减少了内核的攻击面。由于 XDP  的早期处理阶段，这有效地向内核的网络堆栈“假装”网络设备从未见过这些数据包。此外，如果堆栈接收路径中的潜在错误被发现并会导致类似“ping of dead”的情况，则可以利用 XDP  立即丢弃此类数据包，而无需重新启动内核或重新启动任何服务。由于能够自动交换此类程序以强制丢弃坏数据包，因此主机上的网络流量甚至不会中断。

  堆栈前处理的另一个用例是，假设内核尚未为数据包分配 `skb` ，BPF  程序可以自由修改数据包，并再次将其“伪装”到堆栈它是由网络设备以这种方式接收的。这允许诸如具有自定义数据包重整和封装协议之类的情况，其中数据包可以在进入 GRO 聚合之前被解封装，否则 GRO 由于不知道自定义协议而无法执行任何类型的聚合。 XDP  还允许将元数据（非数据包数据）推送到数据包前面。这对于正常的内核堆栈是“不可见的”，可以进行 GRO 聚合（用于匹配元数据），然后与 tc  ingress BPF 程序协调处理，其中它具有可用于例如的 `skb` 上下文。设置各种skb字段。

- 流量采样、监测

  XDP  还可以用于数据包监控、采样或任何其他网络分析等情况，例如，作为路径中或终端主机上的中间节点的一部分，也与前面提到的用例相结合。对于复杂的数据包分析，XDP 提供了一种工具，可以有效地将网络数据包（截断或具有完整负载）和自定义元数据推送到从 Linux 性能基础设施提供到用户空间应用程序的快速无锁每 CPU 内存映射环形缓冲区中。这还允许仅分析流的初始数据并一旦确定为绕过监控的良好流量的情况。由于 BPF  带来的灵活性，这允许实现任何类型的自定义监控或采样。

XDP BPF 生产使用的一个例子是 Facebook 的 SHIV 和 Droplet 基础设施，它们实施了 L4 负载平衡和 DDoS  对策。将他们的生产基础设施从 netfilter 的 IPVS（IP 虚拟服务器）迁移到 XDP BPF，与之前的 IPVS  设置相比，速度提高了 10 倍。这是在 netdev 2.1 会议上首次提出的：

- Slides: https://netdevconf.info/2.1/slides/apr6/zhou-netdev-xdp-2017.pdf
- Video: https://youtu.be/YEU2ClcGqts

另一个例子是将 XDP 集成到 Cloudflare 的 DDoS 缓解管道中，该管道最初使用 cBPF 而不是 eBPF 通过 iptables 的 `xt_bpf` 模块进行攻击签名匹配。由于使用  iptables，这会导致受到攻击时出现严重的性能问题，用户空间旁路解决方案被认为是必要的，但也存在缺点，例如需要忙于轮询 NIC  以及将昂贵的数据包重新注入到内核堆栈中。通过直接在内核内部进行高性能可编程数据包处理，向 eBPF 和 XDP 的迁移结合了两全其美：

- Slides: https://netdevconf.info/2.1/slides/apr6/bertin_Netdev-XDP.pdf
- Video: https://youtu.be/7OuOukmuivg

### XDP操作模式

XDP 具有三种操作模式，其中“本机”XDP 是默认模式。当谈论 XDP 时，通常会暗示这种模式。

- 原生 XDP
  - 这是默认模式，XDP BPF 程序直接从网络驱动程序的早期接收路径运行。最广泛使用的 10G 及更高版本的 NIC 已支持本机 XDP。
- 卸载XDP
  - 在卸载XDP模式下，XDP BPF程序直接卸载到NIC中，而不是在主机CPU上执行。因此，本已极低的每数据包成本完全脱离主机 CPU 并在 NIC  上执行，从而提供比在本机 XDP 中运行更高的性能。这种卸载通常由包含多线程、多核流处理器的 SmartNIC 实现，其中内核 JIT 编译器将 BPF 转换为后者的本机指令。支持卸载 XDP 的驱动程序通常也支持本机 XDP，适用于某些 BPF  帮助程序可能尚不可用或仅可用于本机模式的情况。
- 通用XDP
  - 对于尚未实现本机或卸载 XDP 的驱动程序，内核提供了通用 XDP  的选项，该选项不需要任何驱动程序更改，因为在网络堆栈之外的较晚时间点运行。此设置主要针对想要根据内核的 XDP API  编写和测试程序的开发人员，并且不会以本机或卸载模式的性能速率运行。对于生产环境中的 XDP 使用，本机模式或卸载模式更适合，也是推荐的运行  XDP 的方式。

### 驱动支持

#### 支持本机 XDP 的驱动程序

下表提供了支持本机 XDP 的驱动程序列表。接口对应的网络驱动名称可以通过如下方式确定：

```bash
# ethtool -i eth0
driver: nfp
[...]
```

| 供应商                     | Driver 驱动           | XDP 支持        |
| -------------------------- | --------------------- | --------------- |
| Amazon 亚马逊              | ena                   | >= 5.6 >= 5.6   |
| Broadcom 博通              | bnxt_en bnxt_cn       | >= 4.11 >= 4.11 |
| Cavium 鱼子酱              | thunderx 雷克斯       | >= 4.12 >= 4.12 |
| Freescale 飞思卡尔         | dpaa2 dpaa2           | >= 5.0 >= 5.0   |
| Intel 英特尔               | ixgbe 伊克斯贝        | >= 4.12 >= 4.12 |
| ixgbevf 伊克斯贝夫夫       | >= 4.17 >= 4.17       |                 |
| i40e i40e                  | >= 4.13 >= 4.13       |                 |
| ice                        | >= 5.5 >= 5.5         |                 |
| Marvell 马维尔             | mvneta 姆夫内塔       | >= 5.5 >= 5.5   |
| Mellanox 梅拉诺克斯        | mlx4 毫升×4           | >= 4.8 >= 4.8   |
| mlx5 毫升×5                | >= 4.9 >= 4.9         |                 |
| Microsoft 微软             | hv_netvsc hv_netvsc   | >= 5.6 >= 5.6   |
| Netronome 网络机           | nfp                   | >= 4.10 >= 4.10 |
| Others 其他的              | virtio_net virtio_net | >= 4.10 >= 4.10 |
| tun/tap 屯/水龙头          | >= 4.14 >= 4.14       |                 |
| bond 纽带                  | >= 5.15 >= 5.15       |                 |
| Qlogic Q逻辑               | qede 盖德             | >= 4.10 >= 4.10 |
| Socionext 索喜科技         | netsec 网络安全       | >= 5.3 >= 5.3   |
| Solarflare 太阳耀斑        | sfc                   | >= 5.5 >= 5.5   |
| Texas Instruments 德州仪器 | cpsw CPSW             |                 |

#### 支持卸载 XDP 的驱动程序

- 网络机
  - nfp [2](https://docs.cilium.io/en/stable/bpf/progtypes/#id2) 国家人口普查2（某些 BPF 辅助功能（例如检索当前 CPU 编号）在卸载设置中将不可用。）

> [!NOTE]
>
> 编写和加载 XDP 程序的示例包含在相应工具下的 [开发工具](https://docs.cilium.io/en/stable/bpf/toolchain/#bpf-dev) 部分中。

## tc (traffic control)

除了 XDP 等其他程序类型之外，BPF 还可以在网络数据路径中的内核 tc（流量控制）层之外使用。在高层次上，将 XDP BPF 程序与 tc BPF 程序进行比较时存在三个主要差异：

- BPF 输入上下文是 `sk_buff` 而不是 `xdp_buff` 。当内核的网络堆栈接收到数据包时，在 XDP 层之后，它会分配一个缓冲区并解析数据包以存储有关数据包的元数据。这种表示形式称为 `sk_buff` 。然后，该结构会在 BPF 输入上下文中公开，以便来自 tc 入口层的 BPF 程序可以使用堆栈从数据包中提取的元数据。这可能很有用，但会带来执行此分配和元数据提取以及处理数据包直到它命中 tc 挂钩的堆栈的相关成本。根据定义， `xdp_buff` 无法访问此元数据，因为 XDP 挂钩是在此工作完成之前调用的。这是导致 XDP 和 tc hook 之间性能差异的重要因素。

  因此，附加到 tc BPF 钩子的 BPF 程序可以读取或写入 skb 的 `mark` 、 `pkt_type` 、 `protocol` 、 `priority` 、 `queue_mapping` 、 `napi_id` 、 `cb[]` 数组、 `hash` 、 `tc_classid` 或 `tc_index` 、vlan元数据、XDP传输自定义元数据和各种其他信息。 tc BPF 中使用的 `struct __sk_buff` BPF 上下文的所有成员都在 `linux/bpf.h` 系统标头中定义。

  一般来说， `sk_buff` 与 `xdp_buff` 具有完全不同的性质，两者都有优点和缺点。例如， `sk_buff`  情况的优点是可以相当直接地破坏其关联的元数据，但是，它还包含大量协议特定信息（例如GSO相关状态），这使得很难简单地通过仅重写数据包数据来切换协议。这是由于堆栈根据元数据处理数据包，而不是每次都访问数据包内容。因此，BPF 辅助函数需要进行额外的转换，同时注意 `sk_buff` 内部也能正确转换。然而， `xdp_buff` 情况不会面临这样的问题，因为它处于如此早期的阶段，内核甚至还没有分配 `sk_buff` ，因此任何类型的数据包重写都可以轻松实现。但是， `xdp_buff` 情况的缺点是 `sk_buff` 元数据在此阶段不可用于修改。不过，通过将自定义元数据从 XDP BPF 传递到 tc BPF 可以克服后者。这样，可以通过根据用例需要运行两种类型的互补程序来克服每种程序类型的限制。

- 与 XDP 相比，tc BPF 程序可以从网络数据路径中的入口点和出口点触发，而 XDP 则只能从入口点触发。

  内核中的两个钩子点 `sch_handle_ingress()` 和 `sch_handle_egress()` 分别由 `__netif_receive_skb_core()` 和 `__dev_queue_xmit()` 触发。后两个是数据路径中的主要接收和传输功能，将 XDP 放在一边，为进出节点的每个网络数据包触发，从而允许在这些挂钩点处对 tc BPF 程序进行完全可见。

  tc BPF 程序不需要任何驱动程序更改，因为它们在网络堆栈中通用层的挂钩点上运行。因此，它们可以连接到任何类型的网络设备。

  虽然这提供了灵活性，但与在本机 XDP 层运行相比，它也牺牲了性能。然而，在 GRO 运行之后，tc BPF  程序仍然最早出现在通用内核的网络数据路径中，但在任何协议处理之前，传统的 iptables 防火墙（例如 iptables PREROUTING 或 nftables 入口钩子）或其他数据包处理发生。同样，在出口处，tc BPF  程序会在将数据包交给驱动程序本身进行传输之前的最后一点执行，这意味着在传统的 iptables 防火墙挂钩（如 iptables  POSTROUTING）之后，但仍在将数据包交给内核的 GSO 引擎之前。

  然而，确实需要更改驱动程序的一个例外是卸载的 tc BPF 程序，通常由 SmartNIC 以与卸载的 XDP 类似的方式提供，只是由于 BPF 输入上下文、辅助函数和判定代码的差异而具有不同的功能集。

在 tc 层运行的 BPF 程序是从 `cls_bpf` 分类器运行的。虽然 tc 术语将 BPF 连接点描述为“分类器”，但这有点误导，因为它低估了 `cls_bpf` 的能力。也就是说，完全可编程的数据包处理器不仅能够读取 `skb` 元数据和数据包数据，还能够任意破坏两者，并通过动作判决终止 tc 处理。因此， `cls_bpf` 可以被视为管理和执行 tc BPF 程序的独立实体。

`cls_bpf` 可以容纳一个或多个tc BPF程序。在 Cilium 部署 `cls_bpf` 程序的情况下，它仅为 `direct-action` 模式下的给定挂钩附加一个程序。通常，在传统的 tc 方案中，分类器和操作模块之间存在分离，其中分类器附加了一个或多个操作，一旦分类器匹配，就会触发这些操作。在现代世界中，在软件数据路径中使用 tc 时，该模型不能很好地扩展复杂的数据包处理。鉴于附加到 `cls_bpf` 的 tc BPF 程序是完全独立的，它们有效地将解析和操作过程融合到一个单元中。由于 `cls_bpf` 的 `direct-action` 模式，它只会返回 tc 操作判决并立即终止处理管道。这允许通过避免动作的线性迭代来在网络数据路径中实现可扩展的可编程数据包处理。 `cls_bpf` 是 tc 层中唯一能够实现这种快速路径的“分类器”模块。

与 XDP BPF 程序一样，tc BPF 程序可以在运行时通过 `cls_bpf` 自动更新，而无需中断任何网络流量或不必重新启动服务。

`cls_bpf` 本身可以附加到的 tc 入口和出口钩子均由名为 `sch_clsact` 的伪 qdisc 管理。这是入口 qdisc 的直接替代品和正确的超集，因为它能够管理入口和出口 tc 挂钩。对于 `__dev_queue_xmit()` 中的 tc 出口钩子，需要强调的是它不是在内核的 qdisc 根锁下执行的。因此，tc 入口和出口钩子都在快速路径中以无锁方式执行。在任何一种情况下，抢占都会被禁用，并且执行发生在 RCU 读取端。

通常在出口处，有附加到网络设备的 qdisc，例如 `sch_mq` 、 `sch_fq` 、 `sch_fq_codel` 或 `sch_htb` ，其中一些是有类 qdisc，包含子类，因此需要数据包分类机制来确定在何处解复用数据包。这是通过调用 `tcf_classify()` 来处理的，它会调用 tc 分类器（如果存在）。在这种情况下也可以附加并使用 `cls_bpf` 。此类操作通常发生在 qdisc 根锁下，并且可能会发生锁争用。 `sch_clsact` qdisc 的出口挂钩出现得更早，但它并不属于这一点，并且完全独立于传统的出口 qdisc 运行。因此，对于像 `sch_htb` 这样的情况， `sch_clsact` qdisc 可以通过 qdisc 根锁之外的 tc BPF 执行繁重的数据包分类，设置 `skb->mark` 或 `skb->priority` 从那里开始， `sch_htb` 仅需要平面映射，而无需在根锁下进行昂贵的数据包分类，从而减少争用。

`sch_clsact` 与 `cls_bpf` 结合使用的情况支持卸载 tc BPF 程序，其中先前加载的 BPF 程序是从 SmartNIC 驱动程序进行 JIT 处理，以便在 NIC 上本机运行。仅支持卸载运行在 `direct-action` 模式下的 `cls_bpf` 程序。 `cls_bpf` 仅支持卸载单个程序，无法卸载多个程序。此外，只有入口钩子支持卸载 BPF 程序。

一个 `cls_bpf` 实例可以在内部保存多个tc BPF程序。如果是这种情况，则 `TC_ACT_UNSPEC` 程序返回代码将继续执行该列表中的下一个 tc BPF 程序。然而，这有一个缺点，即多个程序需要一遍又一遍地解析数据包，从而导致性能下降。

### BPF 程序返回码

tc 入口和出口钩子共享 tc BPF 程序可以使用的相同操作返回判决。它们在 `linux/pkt_cls.h` 系统标头中定义：

```bash
#define TC_ACT_UNSPEC         (-1)
#define TC_ACT_OK               0
#define TC_ACT_SHOT             2
#define TC_ACT_STOLEN           4
#define TC_ACT_REDIRECT         7
```

系统头文件中还有一些可用的操作 `TC_ACT_*` 判决，它们也在两个钩子中使用。然而，它们与上面的具有相同的语义。意思是，从 tc BPF 的角度来看， `TC_ACT_OK` 和 `TC_ACT_RECLASSIFY` 具有相同的语义，以及三个 `TC_ACT_STOLEN` 、 `TC_ACT_QUEUED` 和 < b5> 操作码。因此，对于这些情况，我们仅描述这两组的 `TC_ACT_OK` 和 `TC_ACT_STOLEN` 操作码。

从 `TC_ACT_UNSPEC` 开始。它具有“未指定操作”的含义，并在三种情况下使用，i) 当附加卸载的 tc BPF 程序并且运行 tc 入口钩子时，卸载程序的 `cls_bpf` 表示将返回 < b2> , ii) 为了在多程序情况下继续 `cls_bpf` 中的下一个tc BPF程序。后者还与来自 i) 点的卸载 tc BPF 程序结合使用，其中 `TC_ACT_UNSPEC` 继续与仅在非卸载情况下运行的下一个 tc BPF 程序一起工作。最后但并非最不重要的一点是，iii) `TC_ACT_UNSPEC` 也用于单个程序情况，以简单地告诉内核继续 `skb` 而不会产生额外的副作用。 `TC_ACT_UNSPEC` 与 `TC_ACT_OK` 操作代码非常相似，两者都将 `skb` 向前传递到入口处的堆栈上层或向下传递到网络分别用于出口传输的设备驱动程序。与 `TC_ACT_OK` 唯一的区别是 `TC_ACT_OK` 根据tc BPF程序集的classid设置 `skb->tc_index` 。后者是通过 BPF 上下文中的 `skb->tc_classid` 从 tc BPF 程序本身中设置的。

`TC_ACT_SHOT` 指示内核丢弃数据包，这意味着网络堆栈的上层永远不会在入口处看到 `skb` ，同样，数据包永远不会在出口处提交传输。 `TC_ACT_SHOT` 和 `TC_ACT_STOLEN` 本质上相似，几乎没有什么区别： `TC_ACT_SHOT` 将向内核指示 `skb` 是通过 `kfree_skb()` 并将 `NET_XMIT_DROP` 返回给调用者以获得即时反馈，而 `TC_ACT_STOLEN` 将通过 `consume_skb()` 释放 `skb` 并假装上层通过 `NET_XMIT_SUCCESS` 判断传输成功。因此，记录 `kfree_skb()` 痕迹的 perf 的丢弃监视器也不会看到来自 `TC_ACT_STOLEN` 的任何丢弃指示，因为其语义是 `skb` 已被“消耗”或排队但肯定不会“丢弃”。

最后但并非最不重要的一点是 `TC_ACT_REDIRECT` 操作也可用于 tc BPF 程序。这允许将 `skb` 与 `bpf_redirect()` 帮助程序一起重定向到同一或另一个设备的入口或出口路径。能够将数据包注入另一个设备的入口或出口方向，从而使 BPF 数据包转发具有充分的灵活性。除了网络设备本身之外，对目标网络设备没有任何要求，不需要在目标设备上运行 `cls_bpf` 的另一个实例或其他此类限制。

### BPF 常见问题解答

本节包含一些不时被问到的与 tc BPF 程序相关的各种问答对。

- Q：`act_bpf` 作为 tc 操作模块怎么样，它仍然相关吗？
- A：不是真的。尽管 `cls_bpf` 和 `act_bpf` 共享 tc BPF 程序的相同功能，但 `cls_bpf` 更加灵活，因为它是 `act_bpf` 的适当超集。 tc 的工作方式是 tc 动作需要附加到 tc 分类器。为了实现与 `cls_bpf` 相同的灵活性，需要将 `act_bpf` 附加到 `cls_matchall` 分类器。顾名思义，这将匹配每个数据包，以便将它们传递给附加的 tc 操作处理。对于 `act_bpf` ，这将导致数据包处理效率低于直接在 `direct-action` 模式下使用 `cls_bpf` 。如果 `act_bpf` 与 `cls_bpf` 或 `cls_matchall` 之外的其他分类器一起使用，那么由于 tc 分类器的操作性质，其性能会更差。这意味着，如果分类器 A 不匹配，则数据包将传递到分类器 B，重新解析数据包等，因此在典型情况下，将进行线性处理，在最坏的情况下，数据包需要遍历 N 个分类器才能找到匹配并执行 `act_bpf` 。因此， `act_bpf` 从来都没有很大的相关性。此外，与 `cls_bpf` 相比， `act_bpf` 也不提供 tc 卸载接口。
- Q：是否建议在 `direct-action` 模式下使用 `cls_bpf` ？
- A：不。答案与上面的答案类似，因为否则无法扩展以进行更复杂的处理。 tc BPF 已经可以高效地完成其自身所需的所有操作，因此除了 `direct-action` 模式之外不需要任何其他内容。
- Q：卸载的 `cls_bpf` 和卸载的 XDP 是否有性能差异？
- A：不。两者都是通过内核中的同一个编译器进行 JIT 处理的，该编译器处理向 SmartNIC 的卸载，并且两者的加载机制也非常相似。因此，BPF 程序被翻译成相同的目标指令集，以便能够在 NIC 上本地运行。 tc BPF 和 XDP BPF  两种程序类型具有一组不同的功能，因此根据用例，由于卸载情况下某些辅助函数的可用性，可能会选择一种而不是另一种。

### tc BPF 的用例

本小节介绍了 tc BPF 程序的一些主要用例。同样，这里的列表并不详尽，考虑到 tc BPF  的可编程性和效率，它可以轻松定制并集成到编排系统中，以解决非常具体的用例。虽然 XDP 的某些用例可能会重叠，但 tc BPF 和 XDP  BPF 大多数情况下是相互补充的，两者也可以同时使用，或者根据哪一个最适合解决给定问题而使用。

- 容器策略执行

  tc BPF 程序适合的一种应用是分别为容器或 Pod  实施策略实施、自定义防火墙或类似的安全措施。在传统情况下，容器隔离是通过网络命名空间实现的，veth  网络设备将主机的初始命名空间与专用容器的命名空间连接起来。由于 veth  对的一端已移至容器的命名空间中，而另一端仍保留在主机的初始命名空间中，因此来自容器的所有网络流量都必须通过面向主机的 veth 设备，从而允许将 tc BPF 程序附加到veth 的 tc 入口和出口挂钩。进入容器的网络流量将通过面向主机的 veth 的 tc  出口挂钩，而来自容器的网络流量将通过面向主机的 veth 的 tc 入口挂钩。

  对于像 veth 设备这样的虚拟设备，XDP 在这种情况下不适合，因为内核仅在 `skb` 上运行，而通用 XDP 有一些限制，它不能与克隆的 `skb` 一起运行。后者在 TCP/IP 堆栈中被大量使用，以便保存数据段以进行重传，而通用 XDP 挂钩将被简单地绕过。此外，通用 XDP 需要对整个 `skb` 进行线性化，从而导致性能严重下降。另一方面，tc BPF 更加灵活，因为它专门针对 `skb` 输入上下文情况，因此不需要应对通用 XDP 的限制。

- 转发和负载均衡

  转发和负载平衡用例与 XDP 非常相似，尽管更针对东西向容器工作负载而不是南北向流量（尽管这两种技术都可以在任何情况下使用）。由于 XDP 仅在入口端可用，因此  tc BPF 程序允许特别适用于出口的进一步用例，例如，基于容器的流量已经可以通过初始命名空间之外的 BPF 在出口端进行 NAT  和负载平衡，例如这对于容器本身来说是透明的。由于内核网络堆栈的性质，出口流量已经基于 `sk_buff` 结构，因此数据包重写和重定向适合在 tc BPF 之外进行。通过利用 `bpf_redirect()` 辅助函数，BPF 可以接管转发逻辑，将数据包推送到另一个网络设备的入口或出口路径。因此，通过利用 tc BPF 作为转发结构，也不再需要使用任何类似桥接的设备。

- 流量采样、监测

  与 XDP 情况一样，流采样和监控可以通过高性能无锁每 CPU 内存映射 perf 环形缓冲区来实现，其中 BPF 程序能够将自定义数据、完整或截断的数据包内容或两者推送给用户空间应用。在 tc BPF 程序中，这是通过 `bpf_skb_event_output()` BPF 辅助函数实现的，该函数具有与 `bpf_xdp_event_output()` 相同的函数签名和语义。鉴于 tc BPF 程序可以附加到入口和出口，而不是 XDP BPF 情况下仅附加到入口，加上两个 tc  挂钩位于（通用）网络堆栈的最低层，这允许对来自特定网络的所有网络流量进行双向监控节点。这可能与 tcpdump 和 Wireshark 使用的  cBPF 情况有些相关，尽管无需克隆 `skb` 并且在可编程性方面更加灵活，例如，BPF 已经可以执行内核聚合，而不是将所有内容推送到用户空间以及推送到环形缓冲区的数据包的自定义注释。后者在 Cilium  中也大量使用，其中可以进一步注释数据包丢弃，以关联容器标签以及必须丢弃给定数据包的原因（例如由于策略违规），以便提供更丰富的上下文。

- 数据包调度程序预处理

  `sch_clsact` 的出口钩子（称为 `sch_handle_egress()` ）在获取内核的 qdisc 根锁之前运行，因此 tc BPF 程序可用于在之前执行所有繁重的数据包分类和修改数据包被传输到真正成熟的 qdisc 中，例如 `sch_htb` 。 `sch_clsact` 与稍后在传输阶段出现的 `sch_htb` 等真实 qdisc 的这种交互可以减少传输时的锁争用，因为 `sch_clsact` 的出口钩子执行时无需加锁。

tc BPF 和 XDP BPF 程序的一个具体示例用户是 Cilium。 Cilium 是开源软件，用于透明地保护使用 Docker 和  Kubernetes 等 Linux 容器管理平台部署的应用程序服务之间的网络连接，并在第 3/4 层和第 7 层运行。Cilium 的核心运行 BPF 以实施策略执行以及负载平衡和监控。

- Slides: https://www.slideshare.net/ThomasGraf5/dockercon-2017-cilium-network-and-application-security-with-bpf-and-xdp
- Video: https://youtu.be/ilKlmTDdFgk
- Github: https://github.com/cilium/cilium

### 驱动支持

由于 tc BPF 程序是从内核的网络堆栈触发的，而不是直接从驱动程序触发的，因此它们不需要任何额外的驱动程序修改，因此可以在任何网络设备上运行。下面列出的唯一例外是将 tc BPF 程序卸载到 NIC。

### 支持卸载 tc BPF 的驱动程序

- **Netronome 网络机**
  - nfp [2](https://docs.cilium.io/en/stable/bpf/progtypes/#id2) 国家人口普查2

> [!NOTE]
>
> 编写和加载 tc BPF 程序的示例包含在相应工具下的  [开发工具](https://docs.cilium.io/en/stable/bpf/toolchain/#bpf-dev) 部分中。



















