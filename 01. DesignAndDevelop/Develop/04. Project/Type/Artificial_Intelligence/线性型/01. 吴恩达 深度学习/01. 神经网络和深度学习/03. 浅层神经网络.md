# 吴恩达深度学习

# 目录

# 浅层神经网络

## 浅层神经网络

前向过程
$$
\left.
	\begin{array}{l}
	x\\
	w\\
	b
	\end{array}
	\right\}
	\implies{z={w}^Tx+b}
	\implies{a = \sigma(z)}\\ 
	\implies{{L}(a,y)}
$$
反向过程
$$
\left.
	\begin{array}{r}
	{da^{[1]} = {d}\sigma(z^{[1]})}\\
	{dW^{[2]}}\\
	{db^{[2]}}\\
	\end{array}
	\right\}
	\impliedby{{dz}^{[2]}={d}(W^{[2]}\alpha^{[1]}+b^{[2]}})
	\impliedby{{{da}^{[2]}} = {d}\sigma(z^{[2]})}\\
	\impliedby{{dL}\left(a^{[2]},y \right)}
$$





## 随机初始化

如果你要初始化成0，由于所有的隐含单元都是对称的，无论你运行梯度下降多久，他们一直计算同样的函数。这没有任何帮助

![image-20221002213429352](03.%20浅层神经网络.assets/image-20221002213429352.png)

$b$没有这个对称的问题（叫做**symmetry breaking problem**，翻译过来叫 对称破坏问题），所以可以把 $b$ 初始化为0

![image-20221002213601174](03.%20浅层神经网络.assets/image-20221002213601174.png)

通常再乘上一个小的数，比如0.01，这样把它初始化为很小的随机数
选择一个较小的w的原因：让z值较小，不要在太两端的位置，便于梯度下降





















