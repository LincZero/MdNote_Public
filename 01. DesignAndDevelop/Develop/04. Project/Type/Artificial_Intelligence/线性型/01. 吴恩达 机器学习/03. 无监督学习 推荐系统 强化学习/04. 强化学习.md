# 吴恩达机器学习

# 目录

# 强化学习（Reinforcement Learning）

## 什么是强化学习

应用，例如：无人驾驶直升机



直升机的位置 --> 如何移动控制杆

状态 s --> 行动 a

x --> y

奖励函数

- 积极回报：当直升机飞得好时
- 消极回报：当直升机飞得不好



应用

- 控制机器人
- 工厂优化
- 金融 (股票) 交易
- 玩游戏 (包括电视游戏)



强化学习的限制

- 在模拟中工作要比真正的机器人容易得多
- 比监督和非监督学习的应用要少得多
- 但是……具有未来应用潜力的令人兴奋的研究方向



## 实例：火星探测器（Mars rover）

火星探测器的例子

比如往左3格，可以采样一份研究价值为100的土壤，往右2格，则可以采样一份研究价值为40的土壤。这时应该怎么决策？

![image-20220921141248526](04.%20强化学习.assets/image-20220921141248526.png)



## 一些术语

一些术语：

| 术语 (中)        | 术语 (英)                                     | 符号            | 补充                                                         |
| ---------------- | --------------------------------------------- | --------------- | ------------------------------------------------------------ |
| 状态             | State                                         | $s$             | 当前状态，例如火星车的位置在强化学习中称为状态，六个位置即六种状态 |
| 动作             | Action                                        | $a$             |                                                              |
| 奖励函数         | Reward function                               | $R(s)$          |                                                              |
| **回报**         | Return                                        | $\text{Return}$ | 奖励的综合（Sum of rewards）。$\text{Return}=R_1+\gamma R_2+\gamma^2 R_3+\cdots$ |
| 折扣系数         | Discount factor                               | $\gamma$        | 取值0~1，用于幂后加权。类似于：利率或货币的时间价值          |
| 策略             | Policy                                        | $\pi(s)$        | 策略函数                                                     |
| 状态动作价值函数 | State Action value function<br />/ Q-function | $Q(s,a)$        | 状态-动作价值函数，也叫 Q-function                           |

一些符号：

- $s$：当前状态
- $a$：当前动作
- $R(s)$：当前状态的奖励
- $s'$：行动$a$后的状态
- $a'$：状态$s'$所对应的行动



### 状态 $s$、动作 $a$、奖励 $R(s)$

状态 动作 奖励 的表示：$(s,a,R(s),s')$

![image-20220921141248526](04.%20强化学习.assets/image-20220921141248526.png)



### 回报、折扣系数 $\gamma$

回报的计算方式：

![image-20220921145227600](04.%20强化学习.assets/image-20220921145227600.png)

如果$\gamma$较大，那基本是利益优先。若$\gamma$较小，则会更倾向于去近一点的地方
当$\gamma=0.5$，且位于状态4时，往左是更佳的决策
当$\gamma=0.3$，且位于状态4时，往右是更佳的决策

![image-20220921145832828](04.%20强化学习.assets/image-20220921145832828.png)



### 策略 $\pi(s)$

决策：强化学习中的策略

我们提出一个策略（Policy），我们希望这个策略能基于状态告诉我们应该干什么
$$
\pi(s)=a\\
\pi(2)=\leftarrow\\
\pi(3)=\leftarrow\\
\pi(4)=\leftarrow\\
\pi(5)=\rightarrow
$$



### 状态动作价值函数 $Q(s,a)$

英文：**State Action value function**，因为常用符号$Q(s,a)$表示，也叫 **Q-function**

- 定义

  - $Q(s,a)$表示，当

    - 开始于状态s
    - 采取了一次动作a
    - 然后**在那之后表现最佳**

    所得到的回报的值

  - 其中$Q^*$也叫做 最优Q函数

- 作用：方便递归计算，然后类似于马尔科夫链那样算收敛

- 注意区分：Q、Return、Reward



比如$Q(2,\rightarrow)$中，策略为往右一格再往左两格，分数为12.5

![image-20220921175355245](04.%20强化学习.assets/image-20220921175355245.png)





## 其他

### 其他例子

![image-20220921151504870](04.%20强化学习.assets/image-20220921151504870.png)



### 马尔科夫决策过程（MDP，Markov Decision Process）

应该就是 马尔科夫链 的那个 马尔科夫

![image-20220921171431094](04.%20强化学习.assets/image-20220921171431094.png)



### 贝尔曼方程（Bellman Equation）

如何计算 状态动作价值函数？

复习一下符号：

- $s$：当前状态
- $a$：当前动作
- $R(s)$：当前状态的奖励
- $s'$：行动$a$后的状态
- $a'$：状态$s'$所对应的行动



贝尔曼方程：是一个递归方程
$$
Q(s,a)=R(s)+\gamma\max_{a'}Q(s',a')\\~\\

例如：\\
\begin{align}
Q(2,\rightarrow)=&R(2)+0.5\max_{a'}Q(3,a')\\
=&0+0.5*25=12.5
\end{align}\\~\\

推导原理：\\
R_1+\gamma R_2+\gamma^2 R_3+\gamma^3 R_4+\cdots\\
=R_1+\gamma[R_2+\gamma R_3+\gamma^2 R_4+\cdots]
$$



### *随机环境（Random (/Stochastic) Environment）（可选）*

Random (/Stochastic) Environment（随机环境）

依然是火星车的例子：
假如你让它向左走，有90%的概率向左走，有10%的概率向右走
假如你让它向右走，有90%的概率向右走，有10%的概率向左走

回报期望 & 贝尔曼方程：
$$
~\begin{align}
\text{回报期望 Expected Return}=&\text{Average}(R_1+\gamma R_2+\gamma^2 R_3+\gamma^3 R_4+\cdots)\\
=&E~[R_1+\gamma R_2+\gamma^2 R_3+\gamma^3 R_4+\cdots]\\~\\
贝尔曼方程：Q(s,a)=&R(s)+\gamma~E~[\max_{a'}Q(s',a')]
\end{align}
$$



### 连续状态空间（Continuous state）

示例 - 连续状态空间应用



之前的例子中，火星车的状态只有六种离散状态。这节将会将它的状态延伸为连续的状态空间

比如车子的位置和朝向的表示：
$$
s=\begin{bmatrix}
x\\
y\\
\theta\\
\dot{x}\\
\dot{y}\\
\dot{\theta}
\end{bmatrix}
$$
比如直升机，还有多了一个方向的位移和两个方向上的旋转
$$
s=\begin{bmatrix}
    x\\
    y\\
    z\\
    \phi\\
    \theta\\
    \omega\\
    \dot{x}\\
    \dot{y}\\
    \dot{z}\\
    \dot{\phi}\\
    \dot{\theta}\\
    \dot{\omega}
\end{bmatrix}
$$


## 实例：登月器（Lunar Lander）

### 例子

登月器着陆
$$
s=\begin{bmatrix}
    x\\
    y\\
    \dot{x}\\
    \dot{y}\\
    \theta\\
    \dot{\theta}\\
    l\\
    r
\end{bmatrix}\\
l：左脚是否着地，r：右脚是否着地
$$
登月器奖励的设置，例如

- 到达着陆点：100-140
- 向/远离 垫 移动的额外奖励
- 坠毁：-100
- 软着陆：+100
- 腿被禁足：+10
- 消防主机：-0.3
- 消防侧推进器：-0.03

对于登月器来说$\gamma$的值挺大的，例如设置$\gamma=0.985$



### DQN算法（Deep Q-Network）/ 状态值函数（State-value function）

学习 状态值函数（Learning the state-value function），该算法也叫 **DQN算法（Deep Q-Network）**



**用神经网络训练模型来学习Q函数**

如何训练强化学习的神经网络？
$$
~\begin{align}
    x=&(s,a)\\
    y=&Q(s,a)\\
    =&R(s)+\gamma \max_{a'}Q(s',a')\\
    \approx &？ f_{W,B}(x)\\
    y^{(1)}=&R(s^{(1)})+\gamma \max_{a'}Q(s'^{(1)},a')\\
    Q_{new}(s,a)\approx& y
\end{align}\\~\\

(s,a,R(s),s')\\
(s^{(1)},a^{(1)},R(s^{(1)}),s'^{(1)})
$$


重放缓冲区（Replay Buffer）



训练：创建1w个示例，用于训练
$$
x=(s,a)\\
y=R(s)+\gamma\max_{a'}Q(s',a')
$$


## 算法改进

### 改进的神经网络架构

每个状态要推理四次，有四个输出单元

![image-20220922152746996](04.%20强化学习.assets/image-20220922152746996.png)

![image-20220922152932786](04.%20强化学习.assets/image-20220922152932786.png)



### $\varepsilon$贪婪策略（Epsilon-greedy policy）

步骤

> 将神经网络随机初始化为$Q(s,a)$的猜测
> 重复 {
>     在月球着陆器上采取行动。获得$(s,a,R(s),s')$
>     存储10,000个最近$(s,a,R(s),s')$元组
>     _
>     训练模型：
>     	使用$x=(s,a)$和$y=R(s)+\gamma\max_{a'}(s',a')$，创建由10,000个示例组成的训练集
>         训练$Q_{new}$以$Q_{new}(s,a)\approx y$。$f_{w,b}(x)\approx y$
> 		设置$Q=Q_{new}$
> } 直到收敛



在状态$s$时，有两个选项：

- 选项一

  采取使$Q(s,a)$最大化的行动$a$

- **选项二（$\varepsilon$贪婪策略，Epsilon-greedy policy）**

  95%采取使$Q(s,a)$最大化的行动$a$（exploitation）

  05%采取随机行动$a$

  其中该例子中$\varepsilon=0.05$，或者说$95\%$贪婪



一个技巧是，开始的时候$\varepsilon$设置较高，然后逐渐减少该值。比如从1.0开始，逐渐减少到0.01



### *小批量（Mini batch，可选）*

除了**强化学习**，也适用于**监督学习**



比如如果有一亿多个数据时，梯度下降会很慢，每一次下降都要计算一亿多个数据

小批量梯度下降的思想是，比如这次下降选择1000个子集，下一次下降再选择另外1000个子集，节约时间

![image-20220922170302133](04.%20强化学习.assets/image-20220922170302133.png)



### *软更新（Soft update，可选）*

除了**强化学习**，也适用于**监督学习**



令$Q:=Q_{new}$，由于$Q(s,a)=Q(x)=f_{W,B}(x)=y$

即令

- $W:=W_{new}$
- $B:=B_{new}$

而**软更新（Soft update）**则是，令

- $W:=0.01W_{new}+0.99W$
- $B:=0.01B_{new}+0.99B$



软更新的作用：能让强化学习算法 更可靠地收敛，不容易振荡或转向或产生其他不良特性



























