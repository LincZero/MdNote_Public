# 吴恩达机器学习

# 目录

# 几种最优化算法

另参考：

- [【CSDN】梯度下降法和牛顿法](https://blog.csdn.net/qq_40626659/article/details/103074035)
- [【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）](https://blog.csdn.net/weixin_30947043/article/details/99309191)
  - [【博客园】【Math】常见的几种最优化方法](https://www.cnblogs.com/maybe2030/p/4751804.html)

## 几种最优化算法

常见的几种最优化算法：梯度下降法、牛顿法、拟牛顿法、共轭梯度法等

### 梯度下降法（GD，Gradient Descent）

#### 梯度下降法（GD）



#### 批量梯度下降（BGD）

在梯度下降的每一步中，我们都用到了所有的训练样本，需要进行求和运算

#### 随机梯度下降（SGD）

SGD和BGD相反，SGD每次更新参数仅用一个样本进行，而BGD是用所有样本。

#### 小批量梯度下降（MBGD）



#### 【比较】各种版本的梯度下降













### 牛顿法（Newton's Method）





#### 拟牛顿法（Quasi-Newton Methods）



































## 牛顿法和梯度下降法

先来比较 牛顿法和梯度下降法

- 梯度下降（Gradient Descent）

  - 优点：最简单 最常用 最直观、实现简单，也被称为是 “最速下降法”（当然不意味着速度最快）

  - 原理

    >   是用于求函数最小值的算法，其步骤为：
    >
    >   - 随机选择一个参数组合，计算损失函数
    >   - 通过方向和补仓，对参数进行更新，找下一个能够让损失函数值更低的参数组合
    >   - 持续迭代直至寻找到一个局部最小值。
    >
    >   因为没有尝试所有的参数组合，所以不能保证寻找到的局部最小值就是全局最小值

  - 分类

    - 批量梯度下降（BGD）：在梯度下降的每一步中，我们都用到了所有的训练样本，需要进行求和运算
    - 随机梯度下降（SGD）：SGD和BGD相反，SGD每次更新参数仅用一个样本进行，而BGD是用所有样本。
    - 小批量梯度下降（MBGD）：是BGD和SGD的中和，每次参数迭代用大于一小于所有的样本。其优点为：比SGD精度高，但可能需要的时间比较长。

- 牛顿法和拟牛顿法（Newton's method & Quasi-Newton Methods）

  - 原理

    > 其实质是对 损失函数 进行求导，寻找能使得损失函数导数为0的解，当损失函数导数为0时，也便找到了最优解（极值点）
    >
    > 其迭代过程是在当前位置x0求该函数的切线，该切线和x轴的交点x1，作为新的x0。
    >
    > 重复这个过程，直到交点和函数的零点重合。此时的参数值就是使得目标函数取得极值的参数值

- 比较

  - 收敛速度

    收敛速度上，牛顿法要比梯度下降法更快，因为其参数更新的步长会比较大，
    在接近最优解时，梯度下降法也容易因步长比较大而产生来回震荡的效果，从而降低了收敛速度。

  - 计算量

    但是在运行过程中，牛顿法的计算量要远大于梯度下降，因为牛顿法要对多个值进行求导运算，而梯度下降仅需要得出方向和步长便能更新参数

