# 吴恩达机器学习

# 目录

# 深入理解前后向传播 (向量化表示)==（笔记未完工）==

## 基础 - 机器学习

### 符号表示

==符号表示==

| 符号                   | 意思                                                         |
| ---------------------- | ------------------------------------------------------------ |
| n                      | 特征的数量                                                   |
| m                      | 样本的数量                                                   |
| $n_2$                  | 下一层特征的数量                                             |
| ▬▬▬▬▬▬▬▬▬▬             |                                                              |
| 右上角加$[~]$的上标    | 表示处于第几个网络层                                         |
| 右上角加$(i)$的上标    | 表示用的是第几个样本（在向量化的公式里是看不到这个表示的）   |
| 右下角数字$j$的下标    | 表示是该网络层的第$j$个单元。是输入层的第$j$个特征。也对应于矩阵或向量中的 “第几行”<br />注意：其中$x_1\not=x^{(1)}_1$，前者表示第一个特征，后者表示第一个特征的第一个样本 |
| ▬▬▬▬▬▬▬▬▬▬             |                                                              |
| $w_j^{[l]}、b_j^{[l]}$ | 表示处于第$l$个神经网络层的第$j$个神经元所使用函数的参数     |
| $\vec x$               | 表示输入层的矢量表示，等价于$\vec a^{[l]}$                   |
| $\vec a^{[l]}$         | 表示序列$l$ (注意从0开始计数) 的神经网络层的输出的矢量表示   |



想要和矩阵对应起来，有两种方案

一种用矩阵：

- 将特征竖向排布、样本横向排布（与表格思维相谬，与列向量相谬，但可能会和程序较匹配）
- $\vec w\times\vec x$的叉乘顺序（也符合变换矩阵在前的规则）

另一种方案就是嵌套，不用矩阵用向量：

- （事实上都是点乘，根本没有用矩阵的必要），$\vec X=[x_i]$是竖向量，$\vec x_1=[x_1^{(j)}]$也是竖向量。$\vec X$就是一个嵌套竖向量，里面的元素也是竖向量

一些理解的建议：先向量化特征，再向量化样本。即多特征单样本多输出作为切入点，而不是单特征多样本多输出

![](03.%20深入理解前后向传播 (向量化表示).assets/L1_week3_1-16642670577931.png)

### 向量化

#### 概念与作用

什么是向量化？

- $去除\text{for}\neq向量化$

- $向量化\neq矩阵乘法$

- 复习一下点乘和叉乘
  $$
  \vec a=\begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}，
  \vec b=\begin{bmatrix}b_1\\b_2\\\vdots\\b_n\end{bmatrix}\\~\\
  
  \begin{align}
      \vec a\cdot\vec b=& \vec a^T\times\vec b\\
      \vec a\cdot\vec b=& \sum^n_{i=1}a_ib_i\\
      \vec a^T\times\vec b=& \begin{bmatrix}\sum^n_{i=1}a_ib_i\end{bmatrix}
  \end{align}
  $$



向量化的目的

- 简化公式

  - $$
    f_{\vec w,b}(\vec x)=w_1x_1+w_2x_2+w_3x_3+\dots+b\\
    可写成\\
    f_{\vec w,b}(\vec x)=\vec w\cdot \vec x+b
    $$

- 简化代码，减少for语句

  - 见后一节

- 加快计算，利用并行资源

  - 对比向量化和非向量化的时间差异（点乘）

    ```python
    import numpy as np 			# 导入numpy库
    a = np.array([1,2,3,4]) 	# 创建一个数据a
    print(a)					# 结果：[1 2 3 4]
    
    import time 				# 导入时间库
    a = np.random.rand(1000000)
    b = np.random.rand(1000000) # 通过round随机得到两个一百万维度的数组
    tic = time.time() 			# 现在测量一下当前时间
    
    # 向量化的版本
    c = np.dot(a,b)
    toc = time.time()
    print("Vectorized version:" + str(1000*(toc-tic)) +"ms") 	# 打印一下向量化的版本的时间。结果：1.5027ms
    
    # 非向量化的版本
    c = 0
    tic = time.time()
    for i in range(1000000):
        c += a[i]*b[i]
    toc = time.time()
    print(c)
    print("For loop:" + str(1000*(toc-tic)) + "ms") 			# 打印for循环的版本的时间。结果：474.2951ms
    ```




#### 三个for

无向量化的情况，对于样本数m和特征数n，分别有1个循环，共2个循环

- 特征数n的循环，可以向量化
  - $$
    f_{\vec w,b}(\vec x)=w_1x_1+w_2x_2+w_3x_3+\dots+b\\
    向量化后：\\
    f_{\vec w,b}(\vec x)=\vec w\cdot \vec x+b
    $$

- 样本数m的循环，可以向量化

  - $$
    Loss函数为\~\~平方误差\~\~为例：\\
    J(\vec w,b)=\frac{1}{2 m} \sum_{i=1}^{m}\left(f_{\vec w,b}\left(x_{(i)}\right)-y_{(i)}\right)^{2}\\
    
    （sigmoid回归Loss函数为例，这里有问题，用空要再算一遍）向量化后：\\
    J(\vec w,b)=\frac{1}{2 m} (A-Y)
    $$

  - $$
    d{{w}_{1}}=\frac{dL}{dz}\frac{dz}{dw_1}=\frac{1}{m}\sum\limits_{i}^{m}{x_{1}^{(i)}}({{a}^{(i)}}-{{y}^{(i)}})\\~\\
    
    向量化后：\\
    \begin{align}
    dZ=& A-Y\\
    dw_1=& \frac 1m\times X\times dz^T\\
    db=& \frac 1m\times \text{np.sum}(dZ)
    \end{align}
    $$

  - 补充：话说推荐系统的协同过滤算法有两个$\sum$，那个能简化掉不？A：TODO：等我有空再写

- 神经网络中，循环遍历每一层。不能向量化

  - 由于不同层的向量尺寸不同，没办法连成一个矩阵。这个for省不了


### 前向传播

唯一目的就是通过 $\vec x$ 计算 $a$



### 反向传播

唯一目的就是通过 $\vec X, \vec y$，计算$\vec W,\vec b$

至于Loss函数和代价函数都是引入的中间概念，**在实际计算当中并不需要计算这两个中间变量的**，一般只知道需要$\vec a$就能算出$\vec W,\vec b$

至于梯度下降，知道$\vec W,\vec b$和$\alpha$后就能直接下降



## 基础 - 深层神经网络

### 核对矩阵的维数

当实现深度神经网络的时候，其中一个我常用的检查代码是否有错的方法就是拿出一张纸过一遍算法中矩阵的维数。

在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率。


$$
~
\begin{align}
    \text{shape}({w^{[l]}})=& w的维度=({n^{[l]}},{{n}^{[l-1]}})=(下一层的维数，前一层的维数)\\
    \text{shape}({b^{[l]}})=& b~的维度=({n^{[l]}},1)~~~~~~~=(下一层的维数，1)\\
    \text{shape}({z^{[l]}})=& z~的维度=~↓\\
    \text{shape}({a^{[l]}})=& a~的维度=({n^{[l]}},1)
\end{align}\\~\\

{dw^{[l]}}和{{w}^{[l]}}维度相同，{{db}^{[l]}}和{b^{[l]}}维度相同，且w和b向量化维度不变，\\但z,a以及x的维度会向量化后发生变化。
$$
向量化后：
$$
~
\begin{align}
    \text{shape}({Z^{[l]}})=& Z的维度=({n^{[l]}},m)\\
    \text{shape}(A^{[l]})=& A的维度=(n^{[l]},m)\\
    \text{shape}(X~~)=&\text{shape}({A}^{[0]})=(n^{[l]},m)\\
\end{align}\\~\\

其中Z^{[l]}可以看成由每一个单独的Z^{[l]}叠加而得到\\
Z^{[l]}=({z^{[l][1]}}，{z^{[l][2]}}，{z^{[l][3]}}，…，{z^{[l][m]}})
$$



# 多种情况

## 单特征 多特征 单输出

符号

| 符号                                                  | 描述        | 展开                                                 |
| ----------------------------------------------------- | ----------- | ---------------------------------------------------- |
| $w$                                                   | 标量        |                                                      |
| $b$                                                   | 标量        |                                                      |
| $x^{(i)}$                                             | 标量        |                                                      |
| $a^{(i)}=\hat y^{(i)}=g(z^{(i)})=g(f_{w,b}(x^{(i)}))$ | 标量        |                                                      |
| $y^{(i)}$                                             | 标量        |                                                      |
| $\vec x$                                              | (1, m) 矩阵 | $\begin{bmatrix}x^{(1)}&\cdots&x^{(m)}\end{bmatrix}$ |
| $\vec a=g(\vec z)=g(f_{w,b}(\vec x))$                 | (1,m) 矩阵  | $\begin{bmatrix}a^{(1)}&\cdots&a^{(m)}\end{bmatrix}$ |
| $\vec y$                                              | (1,m) 向量  | $\begin{bmatrix}y^{(1)}&\cdots&y^{(m)}\end{bmatrix}$ |



### 线性回归 - 非向量化表示

| 内容                        | 符号                                    | 公式（不展开）                                        | 公式（展开）                                                 |
| --------------------------- | --------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| 预测模型 (线性回归模型)     | $z^{(i)}=\hat y^{(i)}=f_{w,b}(x^{(i)})$ | $wx^{(i)}+b$                                          | $wx^{(i)}+b$                                                 |
| 激活函数 (无)               | $a^{(i)}=g(z^{(i)})$                    | $z^{(i)}$                                             | $wx^{(i)}+b$                                                 |
|                             |                                         |                                                       |                                                              |
| ~~损失函数 (平方误差函数)~~ | $L(a^{(i)},y^{(i)})$                    | $(a^{(i)}-y^{(i)})^2$                                 | $(wx^{(i)}+b-y^{(i)})^2$                                     |
| ~~代价函数~~                | $J(a_1,\cdots,a_m,y_1,\cdots,y_m)$      | $\frac 1 {2m}\sum_{i=1}^m(L(a^{(i)},y^{(i)}))$        | $略$                                                         |
|                             |                                         |                                                       |                                                              |
| w偏导数                     | $dw=\frac\partial{\partial w}J(w,b)$    | $\frac1m∑^m_{i=1}\left(a^{(i)}−y^{(i)}\right)x^{(i)}$ | $\frac1m∑^m_{i=1}\left(f_{w,b}(x^{(i)})−y^{(i)}\right)x^{(i)}$ |
| b偏导数                     | $db=\frac\partial{\partial b}J(w,b)$    | $\frac1m∑^m_{i=1}\left(a^{(i)}−y^{(i)}\right)$        | $\frac1m∑^m_{i=1}\left(f_{w,b}(x^{(i)})−y^{(i)}\right)$      |



### 线性回归 - 向量化表示

（不会出现$~^{(i)}$的上标）

| 内容                        | 符号                                 | 公式（不展开）                               | 公式（展开） |
| --------------------------- | ------------------------------------ | -------------------------------------------- | ------------ |
| 预测模型 (线性回归模型)     | $\vec z=f_{w,b}(\vec x)$             | $w\vec x+b$                                  | $w\vec x+b$  |
| 激活函数 (无)               | $\vec a=g(\vec z)$                   | $\vec z$                                     | $w\vec x+b$  |
|                             |                                      |                                              |              |
| ~~损失函数 (平方误差函数)~~ | $L(\vec a,\vec y)$                   | $(\vec a-\vec y)\cdot(\vec a-\vec y)$        | $略$         |
| ~~代价函数~~                | $J(\vec a,\vec y)$                   | $\frac 1 {2m}\sum_{i=1}^m(L(\vec a,\vec y))$ | $略$         |
|                             |                                      |                                              |              |
| w偏导数                     | $dw=\frac\partial{\partial w}J(w,b)$ | $\frac1m∑\left(\vec a−\vec y\right)\vec x$   | $略$         |
| b偏导数                     | $db=\frac\partial{\partial b}J(w,b)$ | $\frac1m∑\left(\vec a−\vec y\right)$         | $略$         |



### 逻辑回归 - 非向量化表示

| 内容                        | 符号                                    | 公式（不展开）                                        | 公式（展开）                                                 |
| --------------------------- | --------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| 预测模型 (逻辑回归模型)     | $z^{(i)}=\hat y^{(i)}=f_{w,b}(x^{(i)})$ | $wx^{(i)}+b$                                          | $wx^{(i)}+b$                                                 |
| 激活函数 (sigmoid)          | $a^{(i)}=g(z^{(i)})$                    | $\sigma(z^{(i)})$                                     | $\sigma(wx^{(i)}+b)$                                         |
|                             |                                         |                                                       |                                                              |
| ~~损失函数 (逻辑回归误差)~~ | $L(a^{(i)},y^{(i)})$                    | $-y^{(i)}\log a^{(i)}-(1+y^{(i)})\log(1-a^{(i)})$     | 略                                                           |
| ~~代价函数~~                | $J(a_1,\cdots,a_m,y_1,\cdots,y_m)$      | $\frac 1 {m}\sum_{i=1}^m(L(a^{(i)},y^{(i)}))$         | 略                                                           |
|                             |                                         |                                                       |                                                              |
| w偏导数                     | $dw=\frac\partial{\partial w}J(w,b)$    | $\frac1m∑^m_{i=1}\left(a^{(i)}−y^{(i)}\right)x^{(i)}$ | $\frac1m∑^m_{i=1}\left(f_{w,b}(x^{(i)})−y^{(i)}\right)x^{(i)}$ |
| b偏导数                     | $db=\frac\partial{\partial b}J(w,b)$    | $\frac1m∑^m_{i=1}\left(a^{(i)}−y^{(i)}\right)$        | $\frac1m∑^m_{i=1}\left(f_{w,b}(x^{(i)})−y^{(i)}\right)$      |



### 逻辑回归 - 向量化表示

（不会出现$~^{(i)}$的上标）

| 内容                        | 符号                                 | 公式（不展开）                                | 公式（展开）        |
| --------------------------- | ------------------------------------ | --------------------------------------------- | ------------------- |
| 预测模型 (逻辑回归模型)     | $\vec z=f_{w,b}(\vec x)$             | $w\vec x+b$                                   | $w\vec x+b$         |
| 激活函数 (sigmoid)          | $\vec a=g(\vec z)$                   | $\sigma(\vec z)$                              | $\sigma(w\vec x+b)$ |
|                             |                                      |                                               |                     |
| ~~损失函数 (逻辑回归误差)~~ | $L(\vec a,\vec y)$                   | $-\vec y\log \vec a-(1+\vec y)\log(1-\vec a)$ | 略                  |
| ~~代价函数~~                | $J(\vec a,\vec y)$                   | $\frac 1 {m}\sum_{i=1}^m(L(\vec a,\vec y))$   | 略                  |
|                             |                                      |                                               |                     |
| w偏导数                     | $dw=\frac\partial{\partial w}J(w,b)$ | $\frac1m∑\left(\vec a−\vec y\right)\vec x$    | $略$                |
| b偏导数                     | $db=\frac\partial{\partial b}J(w,b)$ | $\frac1m∑\left(\vec a−\vec y\right)$          | $略$                |



## 多特征 单样本 单输出

![](03.%20深入理解前后向传播 (向量化表示).assets/L1_week3_1-16642670577931.png)

![](03.%20深入理解前后向传播 (向量化表示).assets/L1_week3_6.png)















## 多特征 多样本 单输出

这里一般分两步，不会一步到位，因为第一步是向量化的乘法
$$
\begin{align}
    z^{[1](i)}=&W^{[1](i)}x^{(i)}+b^{[1](i)}\\
    a^{[1](i)}=&\sigma(z^{[1](i)})\\
    z^{[2](i)}=&W^{[2](i)}a^{[1](i)}+b^{[2](i)}\\
    a^{[2](i)}=&\sigma(z^{[2](i)})
\end{align}
$$

符号

| 符号                                                       | 描述        | 展开                                                         |
| ---------------------------------------------------------- | ----------- | ------------------------------------------------------------ |
| $\vec w$                                                   | (n,1) 向量  | $\begin{bmatrix}w_1&\cdots&w_n\end{bmatrix}$                 |
| $\vec W_{广播}$                                            | (m,n) 向量  | $\begin{bmatrix}\vdots&&\vdots\\w_1&\cdots&w_n\\\vdots&&\vdots\end{bmatrix}$（广播） |
| $b$                                                        | 标量        |                                                              |
| $x^{(i)}_j$                                                | 标量        |                                                              |
| $a^{(i)}=\hat y^{(i)}=g(z^{(i)})=g(f_{\vec w,b}(x^{(i)}))$ | 标量        |                                                              |
| $y^{(i)}$                                                  | 标量        |                                                              |
| $\vec x_j$                                                 | (1, n) 向量 | $\begin{bmatrix}x_1&\cdots&x_n\end{bmatrix}^T$               |
| $\vec x^{(i)}$                                             | (m,1) 向量  | $\begin{bmatrix}x^{(1)}&\cdots&x^{(m)}\end{bmatrix}$         |
| $\vec X$                                                   | (m,n) 向量  | $\begin{bmatrix}x_1^{(1)}&\cdots&x_1^{(m)}\\\vdots&&\vdots\\x_n^{(1)}&\cdots&x_n^{(m)}\end{bmatrix}$ |
| $\vec a=g(\vec z)=g(f_{\vec w,b}(\vec x))$                 | (m,1) 向量  | $\begin{bmatrix}a^{(1)}&\cdots&a^{(m)}\end{bmatrix}$         |
| $\vec y$                                                   | (m,1) 向量  | $\begin{bmatrix}y^{(1)}&\cdots&y^{(m)}\end{bmatrix}$         |



### 线性回归 - 非向量化表示

| 内容                    | 符号                                           | 公式（不展开）                                 | 公式（展开）                         |
| ----------------------- | ---------------------------------------------- | ---------------------------------------------- | ------------------------------------ |
| 预测模型 (线性回归模型) | $z^{(i)}=\hat y^{(i)}=f_{\vec w,b}(x_j^{(i)})$ | $\sum_{j=1}^n (w_jx_j^{(i)})+b$                | $w_1x_1^{(i)}+w_2x_2^{(i)}+\cdots+b$ |
| 激活函数 (无)           | $a^{(i)}=g(z^{(i)})$                           | $z^{(i)}$                                      | $wx^{(i)}+b$                         |
|                         |                                                |                                                |                                      |
| 损失函数 (平方误差函数) | $L(a^{(i)},y^{(i)})$                           | $(a^{(i)}-y^{(i)})^2$                          | $(wx^{(i)}+b-y^{(i)})^2$             |
| 代价函数                | $J(\vec a,\vec y)$                             | $\frac 1 {2m}\sum_{i=1}^m(L(a^{(i)},y^{(i)}))$ | 略                                   |
|                         |                                                |                                                |                                      |
|                         |                                                |                                                |                                      |
|                         |                                                |                                                |                                      |



### 线性回归 - 向量化表示

| 内容                    | 符号                          | 公式（不展开）                                               | 公式（展开）             |
| ----------------------- | ----------------------------- | ------------------------------------------------------------ | ------------------------ |
| 预测模型 (线性回归模型) | $\vec z=f_{\vec w,b}(\vec X)$ | $=\vec w\cdot\vec x^{(i)}+b$<br />$=\vec W_{广播}\cdot\vec X+\vec B_{广播}$ |                          |
| 激活函数 (无)           | $a^{(i)}=g(z^{(i)})$          | $z^{(i)}$                                                    | $wx^{(i)}+b$             |
|                         |                               |                                                              |                          |
| 损失函数 (平方误差函数) | $L(a^{(i)},y^{(i)})$          | $(a^{(i)}-y^{(i)})^2$                                        | $(wx^{(i)}+b-y^{(i)})^2$ |
| 代价函数                | $J(\vec a,\vec y)$            | $\frac 1 {2m}\sum_{i=1}^m(L(a^{(i)},y^{(i)}))$               | 略                       |
|                         |                               |                                                              |                          |
|                         |                               |                                                              |                          |
|                         |                               |                                                              |                          |



### 逻辑回归 - 向量化表示

| 内容                    | 符号                                              | 公式（不展开）                     | 公式（展开）             |
| ----------------------- | ------------------------------------------------- | ---------------------------------- | ------------------------ |
| 预测模型 (线性回归模型) | $z^{(i)}=\hat y^{(i)}=f_{\vec w,b}(\vec x^{(i)})$ | $\vec w\vec x^{(i)}+b$             | $\vec w\vec X+b$         |
| 激活函数 (无)           | $a^{(i)}=g(z^{(i)})$                              | $z^{(i)}$                          | $wx^{(i)}+b$             |
|                         |                                                   |                                    |                          |
| 损失函数 (平方误差函数) | $L(a^{(i)},y^{(i)})$                              | $(a-y)^2$                          | $(wx^{(i)}+b-y^{(i)})^2$ |
| 代价函数                | $J(\vec a,\vec y)$                                | $\frac 1 {2m}\sum_{i=1}^m(L(a,y))$ | 略                       |
|                         |                                                   |                                    |                          |
|                         |                                                   |                                    |                          |
|                         |                                                   |                                    |                          |



## 多特征 单样本 多输出

即将$x_1,x_2,x_3$都分别看成是一个值，没有$x^{(1)},x^{(2)},x^{(3)}$，只有$x^{(1)}$
不谈模型训练只谈前向传播



对于下面的这个表格，我们只需要前两行，后面的不需要

| 内容          | 符号 | 公式（不展开） | 公式（展开） |
| ------------- | ---- | -------------- | ------------ |
| 预测模型      |      |                |              |
| 激活函数      |      |                |              |
| ~~损失函数~~  |      |                |              |
| ~~代价函数~~  |      |                |              |
| ~~w_j偏导数~~ |      |                |              |
| ~~b偏导数~~   |      |                |              |



符号





### 向量化表示

| 内容          | 符号                          | 公式（不展开）               | 公式（展开） |
| ------------- | ----------------------------- | ---------------------------- | ------------ |
| 预测模型      | $\vec z=f_{\vec w,b}(\vec x)$ | $=\vec w\cdot\vec x^{(i)}+b$ |              |
| 激活函数      |                               |                              |              |
| ~~损失函数~~  |                               |                              |              |
| ~~代价函数~~  |                               |                              |              |
| ~~w_j偏导数~~ |                               |                              |              |
| ~~b偏导数~~   |                               |                              |              |



## ==多特征 多样本 多输出==

### ~~旧版，错误版~~

（前向过程：该过程中每个特征只有一个数，$\vec x$是一个列向量）

该例子使用向量化

![w600](03.%20深入理解前后向传播 (向量化表示).assets/L1_week3_2.png)

![w800](03.%20深入理解前后向传播 (向量化表示).assets/L1_week3_8.png)

符号（第一层，第二层和前面的多特征无中间层的情况基本一样）

| 第一层符号                                            | 描述          | 展开                                                         |
| ----------------------------------------------------- | ------------- | ------------------------------------------------------------ |
| $\vec W^{[1]}$                                        | $(n,n_2)$向量 | $\begin{bmatrix}w_1&\cdots&w_n\\\vdots&&\vdots\\w_{1,n_2}&\cdots&w_{n,n_2}\end{bmatrix}$ |
| $\vec b^{[1]}$                                        | $(n,1)$向量   | $\begin{bmatrix}b_1\\\vdots\\b_n\end{bmatrix}$               |
| $x^{(i)}_j=a_j^{(i)[0]}$                              | 标量          |                                                              |
| $a^{(i)[1]}=g(z^{(i)[1]})=g(f_{\vec w,b}(x^{(i)}_j))$ | 标量          |                                                              |
| $\vec x_j$                                            | $(1, n)$向量  | $\begin{bmatrix}x_1&\cdots&x_n\end{bmatrix}$                 |
| $\vec x^{(i)}$                                        | $(m,1)$向量   | $\begin{bmatrix}x^{(1)}&\cdots&x^{(m)}\end{bmatrix}^T$       |
| $\vec x$                                              | $(m,n)$向量   | $\begin{bmatrix}x_1^{(1)}&\cdots&x_1^{(m)}\\\vdots&&\vdots\\x_n^{(1)}&\cdots&x_n^{(m)}\end{bmatrix}$ |
| $\vec a^{[1]}=g(\vec z)=g(f_{\vec w,b}(\vec x))$      | $(m,n_2)$向量 | $\begin{bmatrix}a^{(1)}&\cdots&a^{(m)}\end{bmatrix}^T$       |
| $\vec A^{[1]}$                                        |               |                                                              |

其中：$\vec W \vec x+\vec b=\vec a$，$shape(n,n_2)\cdot shape(m,n)=shape(m,n_2)$，很合理



向量参数的shape
$$
\begin{align}
	\text{shape}(a^{[0]})=&~(n^{[0]},1)\\
	\text{shape}(W^{[1]})=&~(n^{[1]},n^{[0]})\\
	\text{shape}(b^{[1]})=&~(n^{[1]},1)
	\\~\\
	
	\text{shape}(a^{[1]})=&~(n^{[1]},1)\\
	\text{shape}(W^{[2]})=&~(n^{[2]},n^{[1]})\\
	\text{shape}(b^{[2]})=&~(n^{[2]},1)
\end{align}
$$
0~1层：3节点 => 4节点
$$
\begin{aligned}
    \vec x=\vec a^{[0]}
	=&\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}a_1^{[0]}\\a_2^{[0]}\\a_3^{[0]}\end{bmatrix}
	\\~-------&-------------\\
    
    
    z^{[1]}=&W^{[1]} \vec x+b^{[1]}\\
    =&\begin{bmatrix}
    	w_{11}^{[1]}&w_{12}^{[1]}&w_{13}^{[1]}\\
    	w_{21}^{[1]}&w_{22}^{[1]}&w_{23}^{[1]}\\
    	w_{31}^{[1]}&w_{32}^{[1]}&w_{33}^{[1]}\\
    	w_{41}^{[1]}&w_{42}^{[1]}&w_{43}^{[1]}
    \end{bmatrix}
    \times
    \begin{bmatrix}a_1^{[0]}\\a_2^{[0]}\\a_3^{[0]}\end{bmatrix}
    +
    \begin{bmatrix}b_1^{[0]}\\b_2^{[0]}\\b_3^{[0]}\\b_4^{[0]}\end{bmatrix}
    =
    \begin{bmatrix}
    	w_{11}^{[1]}a_1^{[0]}+w_{12}^{[1]}a_2^{[0]}+w_{13}^{[1]}a_3^{[0]}+b_1^{[1]}\\
    	w_{21}^{[1]}a_1^{[0]}+w_{22}^{[1]}a_2^{[0]}+w_{23}^{[1]}a_3^{[0]}+b_2^{[1]}\\
    	w_{31}^{[1]}a_1^{[0]}+w_{32}^{[1]}a_2^{[0]}+w_{33}^{[1]}a_3^{[0]}+b_3^{[1]}\\
    	w_{41}^{[1]}a_1^{[0]}+w_{42}^{[1]}a_2^{[0]}+w_{43}^{[1]}a_3^{[0]}+b_4^{[1]}
    \end{bmatrix}\\
    =&
    \begin{bmatrix}z^{[1]}_1\\z^{[1]}_2\\z^{[1]}_3\\z^{[1]}_4\end{bmatrix}
    \\~-------&-------------\\
    
    
    \vec y^{[1]}=\vec a^{[1]}
	=&\sigma(\vec z^{[1]})=\sigma(\vec W^{[1]} \vec a^{[0]}+\vec b^{[1]})\\
    =&\begin{bmatrix}a^{[1]}_1\\a^{[1]}_2\\a^{[1]}_3\\a^{[1]}_4\end{bmatrix}
\end{aligned}
$$
1~2层：4节点 => 1节点
$$
\begin{aligned}
	\vec y^{[2]}=\vec a^{[2]}
	=&\sigma(\vec z^{[2]})=\sigma(\vec W^{[2]}\cdot \vec a^{[1]}+\vec b^{[2]})\\
    =&\begin{bmatrix}a^{[2]}_1\\a^{[2]}_2\\a^{[2]}_3\\a^{[2]}_4\end{bmatrix}
    \\~-------&-------------\\
    
    
    \vec z^{[2]}=&\vec W^{[2]} \vec a^{[1]}+b^{[2]}\\
    =&\begin{bmatrix}w^{[2]}_1&w^{[2]}_2&w^{[2]}_3&w^{[2]}_4\end{bmatrix}
    \times
    \begin{bmatrix}a^{[1]}_1\\a^{[1]}_2\\a^{[1]}_3\\a^{[1]}_4\end{bmatrix}
    +
    b^{[2]}\\
    =&w_1^{[2]}a_1^{[1]}+w_2^{[2]}a_2^{[1]}+w_3^{[2]}a_3^{[1]}+w_4^{[2]}a_4^{[1]}+b^{[2]}
    \\~-------&-------------\\
    
    
    \vec y^{[2]}=\vec a^{[2]}
    =&\sigma(\vec z^{[2]})=\sigma(\vec W^{[2]}\ \vec a^{[1]}+\vec b^{[2]})\\
    =&\begin{bmatrix}a^{[2]}_1\end{bmatrix}
    \\~-------&-------------\\
    
    
    J=&\sum^m_{i=1} L(y_i,a_1^{[2](i)})
\end{aligned}
$$



其他写法

或者用另一种写法来表示$W$
$$
\left[
		\begin{array}{c}
		z^{[1]}_{1}\\
		z^{[1]}_{2}\\
		z^{[1]}_{3}\\
		z^{[1]}_{4}\\
		\end{array}
		\right]
		 =
	\overbrace{
	\left[
		\begin{array}{c}
		...W^{[1]T}_{1}...\\
		...W^{[1]T}_{2}...\\
		...W^{[1]T}_{3}...\\
		...W^{[1]T}_{4}...
		\end{array}
		\right]
		}^{W^{[1]}}
		*
	\overbrace{
	\left[
		\begin{array}{c}
		x_1\\
		x_2\\
		x_3\\
		\end{array}
		\right]
		}^{input}
		+
	\overbrace{
	\left[
		\begin{array}{c}
		b^{[1]}_1\\
		b^{[1]}_2\\
		b^{[1]}_3\\
		b^{[1]}_4\\
		\end{array}
		\right]
		}^{b^{[1]}}
$$

### 新版

![w800](03.%20深入理解前后向传播 (向量化表示).assets/L1_week3_8.png)

用维数检测
$$
Z^{[1]}=W^{[1]}A^{[0]}:(4,3)(3,100)=(4,100)\\
A^{[1]}:Z^{[1]}:(4,100)
$$
展开
$$
\begin{bmatrix}
	w_{11}& w_{12}& w_{13}\\
	w_{21}& w_{22}& w_{23}\\
	w_{31}& w_{32}& w_{33}\\
	w_{41}& w_{42}& w_{43}
\end{bmatrix}
\times

\overbrace{
    \begin{bmatrix}
        x_1^{(1)}& x_1^{(2)}& \cdots\\
        x_2^{(1)}& x_2^{(2)}& \cdots\\
        x_3^{(1)}& x_3^{(2)}& \cdots
    \end{bmatrix}
}^m
=

\overbrace{
    \begin{bmatrix}
        z_1^{(1)}& z_1^{(2)}& \cdots\\
        z_2^{(1)}& z_2^{(2)}& \cdots\\
        z_3^{(1)}& z_3^{(2)}& \cdots\\
        z_4^{(1)}& z_4^{(2)}& \cdots
    \end{bmatrix}
}^m
$$


# 0

## 多样本向量化

（该过程中每个特征有m个样本，$\vec x$是一个矩阵，高m宽n）

![w800](03.%20深入理解前后向传播 (向量化表示).assets/L1_week3_8.png)

输入层
$$
x =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$
第一层网络层的中间变量
$$
Z^{[1]} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$
第一层网络层的输出
$$
A^{[1]} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		\alpha^{[1](1)} & \alpha^{[1](2)} & \cdots & \alpha^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$
以此类推
$$
\left.
		\begin{array}{r}
		\text{$z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1]}$}\\
		\text{$\alpha^{[1](i)} = \sigma(z^{[1](i)})$}\\
		\text{$z^{[2](i)} = W^{[2](i)}\alpha^{[1](i)} + b^{[2]}$}\\
		\text{$\alpha^{[2](i)} = \sigma(z^{[2](i)})$}\\
		\end{array}
		\right\}
		\implies
		\begin{cases}
		\text{$A^{[1]} = \sigma(z^{[1]})$}\\
		\text{$z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$}\\ 
		\text{$A^{[2]} = \sigma(z^{[2]})$}\\ 
		\end{cases}
$$



该例子中为求简化省略了$+b$的步骤，$z=w\cdot x$

现在 $W^{[1]}$ 是一个矩阵，$x^{(1)},x^{(2)},x^{(3)}$都是列向量，矩阵乘以列向量得到列向量，下面将它们用图形直观的表示出来：
$$
W^{[1]}  x =
		\left[
		\begin{array}{ccc}
		\cdots \\
		\cdots \\
		\cdots \\
		\end{array}
		\right]
		
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		x^{(1)} & x^{(2)} & x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		w^{(1)}x^{(1)} & w^{(1)}x^{(2)} & w^{(1)}x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=\\
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		z^{[1](1)} & z^{[1](2)} & z^{[1](3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		Z^{[1]}
$$



其他写法
$$
W^{[1]}  x =
		\left[
		\begin{array}{ccc}
		\cdots \\
		\cdots \\
		\cdots \\
		\end{array}
		\right]
		
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		x^{(1)} & x^{(2)} & x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		w^{(1)}x^{(1)} & w^{(1)}x^{(2)} & w^{(1)}x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=\\
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		z^{[1](1)} & z^{[1](2)} & z^{[1](3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		Z^{[1]}
$$

## 多特征有中间层 - 反向传播

**==吴恩达老师认为反向传播的推导是机器学习领域最难的数学推导之一==，矩阵的导数要用链式法则来求**
（我也觉得这部分有点难了，这也许也是这章是 “选修” 的原因）
如果这章内容掌握不了也没大的关系，只要有这种直觉就可以了。
还有一点，就是**初始化你的神经网络的权重，不要都是0，而是随机初始化**，下一章将详细介绍原因。

![image-20220925212615658](03.%20深入理解前后向传播 (向量化表示).assets/image-20220925212615658.png)



















## 随机初始化

**初始化你的神经网络的权重，不要都是0，而是随机初始化**

在逻辑回归中，将权重参数初始化为0是可行的。但在神经网络中，初始化为0会得不到预期结果















































