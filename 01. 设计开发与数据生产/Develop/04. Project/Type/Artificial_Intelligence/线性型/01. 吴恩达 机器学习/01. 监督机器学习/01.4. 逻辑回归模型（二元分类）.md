# 吴恩达机器学习

# 目录

# 逻辑回归模型（Logistic Regression）逻辑回归

## 动机与目的

- 作用：可以用于解决分类问题。之前说的线性回归和多项式回归则是解决预测问题，而不能解决分类问题
- 例如：找出邮件是否广告、预测肿瘤是恶性还是良性
- 补充
  - 如果分类只有两种，比如回答是或不是，则称之为**二元分类**（Binary Classification）
  - 单/多变量逻辑回归

## ① 模型定义 - Sigmoid函数

### 图像

不能用线性回归模型来解决监督模型的分类问题

![image-20220907080336525](01.4. 逻辑回归模型.assets/image-20220907080336525.png)

一般用数学的Sigmoid函数来构建逻辑回归模型

![image-20220907080918806](01.4. 逻辑回归模型.assets/image-20220907080918806.png)

### 公式

**Sigmoid函数**
$$
g(z)=\frac{1}{1+e^{-z}}，~g(z)\sub (0,1)\\
z=\vec w\cdot\vec x+b
$$
结合模型得到**逻辑函数**（logistic function）
$$
令~z=\vec w\cdot\vec x+b\\
得到逻辑函数:\\
f_{\vec w,b}(\vec x)=g(\vec w\cdot \vec x+b)=\frac 1{1+e^{-(\vec w\cdot \vec x+b)}}\\=P(y=1|x;\vec w,b)
\\~\\
其中：\\
z换元是为了偏移和缩放，令z\sub[0,+\infty)和图像美观\\
x是肿瘤大小\\y轴中是0表示肿瘤良性、1表示肿瘤为恶性\\
P是得到1个概率,y是0.7则表示肿瘤有70\%可能是恶性
$$

### 代码

代码运行结果

![image-20220907082117975](01.4. 逻辑回归模型.assets/image-20220907082117975.png)

### 【概念补充】决策边界（Decision Boundary）

算法：根据Z=0时算出来的，Z=0时，即表示在Sigmoid函数的中间部分

> #### 单参数线性逻辑回归  的决策边界

（这个图也可以画成一维的。将该图y轴缩小无限倍 坍塌成线即可，此时决策边界是一个点）

![image-20220907082117975](01.4. 逻辑回归模型.assets/image-20220907082117975.png)

> #### 二线性元逻辑回归  的决策边界

![image-20220907093638552](01.4. 逻辑回归模型.assets/image-20220907093638552.png)

> #### 二元非线性逻辑回归  的决策边界

![image-20220907093903744](01.4. 逻辑回归模型.assets/image-20220907093903744.png)

椭圆和其他形状的决策边界也同理

## ② 逻辑回归中的Loss函数

这是一个二元逻辑回归的训练集

| 肿瘤大小 cm | 患者的年龄 | 是否恶性 |
| ----------- | ---------- | -------- |
| 10          | 52         | 1        |
| 2           | 73         | 0        |
| 5           | 55         | 0        |
| 12          | 49         | 1        |
| ……          | ……         | ……       |

### 为什么不用平方误差代价函数

Q：能否用平方误差作为代价函数？
A：否，原因如下
$$
线性回归模型一般用平方误差代价函数：\\
J\left(w, b\right)=
\frac{1}{2 m} \sum_{i=1}^{m}\left(f_{w,b}\left(x_{i}\right)-y_{i}\right)^{2}\\
该代价函数是凸函数（convex），能用梯度下降来最优化\\
但是如果代价回归函数也用平方误差代价函数，其代价函数不是凸函数，有多个极小值
$$

![image-20220907095804899](01.4. 逻辑回归模型.assets/image-20220907095804899.png)

### 新Loss函数（分段版）公式

我们需要一个**Loss函数**
$$
Loss：L(f_{\vec w,b}(x_i),y_i)\\
Loss函数以预测值和实际值作为输入，输出损失值L\\~\\
例如：在平方误差代价函数中：\\
L(f_{\vec w,b}(x_i),y_i)=(f_{\vec w,b}(x_i)-y_i)^2
$$
我们最好能找到一个新的Loss函数，使代价函数变成凸函数，以便于收敛 找最优值

逻辑函数的Loss函数如下
$$
L(f_{\vec w,b}(x_i),y_i)=
\begin{cases}
-\log(~~~~~~f_{\vec w,b}(\vec x_i)) & \text{ if } y_i= 1\\ 
-\log(1-f_{\vec w,b}(\vec x_i)) & \text{ if } y_i= 0
\end{cases}
$$

### 新Loss函数（分段版）图像

蓝线部分是Loss函数的图像，根据实际值不同会有两个图像。横轴是预测值，纵轴是代价



当$y_i=1$，即实际值为1时

- 公式：$L(f_{\vec w,b}(x_i))=-\log(f_{\vec w,b}(\vec x_i))$

- 若$f(x_i)=0.001$，预测非常错误，则造成非常非常高的损失

- 若$f(x_i)=1$，        预测完全正确，则损失为0

- 图像如下

  ![image-20220907101058100](01.4. 逻辑回归模型（二元分类）.assets/image-20220907101058100.png)



当$y_i=0$，即实际值为0时

- 公式：$L(f_{\vec w,b}(x_i))=-\log(1-f_{\vec w,b}(\vec x_i))$

- 若$f(x_i)=0.001$，预测非常错误，则造成非常非常高的损失

- 若$f(x_i)=0$，        预测完全正确，则损失为0

- 图像如下

  ![image-20220907101515133](01.4. 逻辑回归模型（二元分类）.assets/image-20220907101515133.png)



### 新Loss函数（单公式）公式

简化Loss函数（Simplified Loss Function）
$$
原Loss函数\\
L(f_{\vec w,b}(x_i),y_i)=
\begin{cases}
    \color{red}-\log(~~~~~~f_{\vec w,b}(\vec x_i)) & \text{ if } y_i= 1\\ 
    \color{orange}-\log(1-f_{\vec w,b}(\vec x_i)) & \text{ if } y_i= 0
\end{cases}
\\~\\

简化版Loss函数\\
L(f_{\vec w,b}(x_i),y_i)=
\color{red}{-y_i\log(f_{\vec w,b}(\vec x_i))}
\color{orange}{-(1-y_i)\log(1-f_{\vec w,b}(\vec x_i))}
\\~\\

其实就是省略了条件判断\\当y=0，红色部分为0；当y=1，橙色部分为0
$$


### 新Loss函数（单公式）图像

这$L(f_{\vec w,b}(x_i),y_i)$就是一个双变量的函数，不过由于$y_i=0/1$，画成三维图来看其实就是两条线而已。
相当于将上面两个函数图像，前后相隔一个单位后叠在一起



## ② 逻辑回归中的代价函数

### 公式

简化代价函数（Simplified Cost Function）

简化完Loss函数后，代入回代价函数中：
$$
\begin{align}
J(\vec w,b)=&~~~\frac1m\sum^m_{i=1}[
	L(f_{\vec w,b}(x_i),y_i)
]\\
=&{\color{red}-}\frac1m\sum^m_{i=1}[
	{\color{red}+y_i\log(f_{\vec w,b}(\vec x_i))}
	{\color{orange}+(1-y_i)\log(1-f_{\vec w,b}(\vec x_i))}
]
\end{align}
$$

### 图像

通过新的Loss函数，代价函数是凸函数，此时可以使用梯度下降方法

代价函数图像如下，顺便比较平均误差代价函数

|                      | 训练集1（符合得较好）                                        | 训练集2（符合得较差）                                        |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 训练集－－－－－－   | ![image-20220907103759184](01.4. 逻辑回归模型.assets/image-20220907103759184.png) | ![image-20220907103802202](01.4. 逻辑回归模型.assets/image-20220907103802202.png) |
| **平均误差代价函数** | ![image-20220907103239493](01.4. 逻辑回归模型.assets/image-20220907103239493.png) | ![image-20220907103825395](01.4. 逻辑回归模型.assets/image-20220907103825395.png) |
| **逻辑回归代价函数** | ![image-20220907103143795](01.4. 逻辑回归模型.assets/image-20220907103143795.png) | ![image-20220907103836222](01.4. 逻辑回归模型.assets/image-20220907103836222.png) |

（一旦预测训练集与回归模型符合得不好，平均误差代价函数就会出问题）

## ③ 梯度下降

### 实现


$$
代价函数：\\
{\color{red}J(\vec w,b)}=-\frac1m\sum^m_{i=1}[
	-y_i\log(f_{\vec w,b}(\vec x_i))
	-(1-y_i)\log(1-f_{\vec w,b}(\vec x_i))
]\\~\\

梯度下降求偏导：\\
\begin{align}
    w_j=&w_j-\alpha\frac\partial{\partial w_j}{\color{red}J(\vec w,b)}\\
    =&w_j-\alpha\frac1m\sum_{i=1}^m({\color{orange}f}_{\vec w,b}(\vec x_i)-y_i)x_{ij}\\
    b=&b-\alpha\frac\partial{\partial b}{\color{red}J(\vec w,b)}\\
    =&b-\alpha\frac1m\sum_{i=1}^m({\color{orange}f}_{\vec w,b}(\vec x_i)-y_i)
\end{align}\\~\\

看上去和线性回归函数一样，但要注意的是这里的f的含义不同，这里的f是逻辑回归\\
Linear~regression：f_{\vec w,b}(x)=\vec w\cdot \vec x+b\\
Logistic~regression：{\color{orange}f}_{\vec w,b}(x)=\frac{1}{1+e^{-z}}
$$

### 偏导的推导过程

$$
计算图：\\
\begin{aligned}
	J(a,y)=&\frac 1m \sum\limits_{i}^{m}(L(a,y))\\
	L(a,y)=&-y\log(a)-(1-y)\log(1-a)\\
	a=&\sigma(z)=\frac1{1-e^z}\\
	z=&w_1x_1+w_2x_2+b
\end{aligned}\\~\\~\\


一些中间变量：\\
\begin{aligned}	
	da=\frac{dL}{da}=&(-\frac{y}{a}+\frac{(1-y)}{(1-a)})
	~~~~~~~~~~~~~~~~~~~~~~=\frac{a-y}{a(1-a)}\\	
	{dz}=\frac{dL}{dz}=&\frac{dL}{da}\frac{da}{dz}=\frac{a-y}{a(1-a)}\cdot a(1-a) = a-y
\end{aligned}\\~\\~\\


用于梯度下降的偏导数：\text{（该例子中每次下降都要遍历m次）}\\
\begin{aligned}
    d{w_1}=&\frac{dL}{dz}\frac{dz}{dw_1}=\frac{1}{m}\sum\limits_{i}^{m}{x_{1}^{(i)}}({a}^{(i)}-{y}^{(i)})\\
    d{w_2}=&\frac{dL}{dz}\frac{dz}{dw_2}=\frac{1}{m}\sum\limits_{i}^{m}{x_{2}^{(i)}}({a}^{(i)}-{y}^{(i)})\\
    db=&\frac{dL}{dz}\frac{dz}{db}~~~=\frac{1}{m}\sum\limits_{i}^{m}{~~~~~~({a}^{(i)}-{y}^{(i)})}\\
\end{aligned}
$$

### 代码（for + 无框架）

代码流程：（假设有m个示例）

这里的代码片段**仅计算一次梯度下降**。如果要梯度下降至收敛，还得再套一层循环

```c
J=0;
dw1=0;
dw2=0;
db=0;

for i = 1 to m:
    z(i) = wx(i)+b;
    a(i) = sigmoid(z(i));
    J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));
    dz(i) = a(i)-y(i);
    dw1 += x1(i)dz(i);
    dw2 += x2(i)dz(i);
    db += dz(i);
                          
J /= m;
dw1 /= m;
dw2 /= m;
db /= m;
                          
w=w-alpha*dw;
b=b-alpha*db;
```

### 代码（向量化 + 无框架）

向量化可以去除for循环，并节省几百倍的时间。
特别是这种循环计算item并sum+=item的类型，一般都可以去并行化。像很多语言的 “高级循环” 语句，都是相似的原理
例如 js 的 reduce等三种高级循环

_

向量化的伪代码：

$Z = w^{T}X + b = np.dot( w.T,X)+b$

$A = \sigma( Z )$

$dZ = A - Y$

${{dw} = \frac{1}{m}*X*dz^{T}\ }$

$db= \frac{1}{m}*np.sum( dZ)$

$w: = w - a*dw$

$b: = b - a*db$

_

- **公式1~2完成了前向传播**，对所有训练样本进行了预测，
- **公式3~5完成了后向传播**，对所有训练样本进行了求导，
- **公式6~7完成了梯度下降**，更新参数。

























