# 李宏毅机器学习深度学习

# 目录

# 局部最小值和鞍点（Local Minimum & Saddle Point）

对应视频：P19
对应pdf：04_Local Minimum And Saddle Point.pdf



## 局部最小值和鞍点

区别：略



## 如何区分二者

这需要一些微积分知识，弹幕说是**黑塞矩阵（也有译作海塞 海森 海辛矩阵的）**

要知道loss function的形状
$$
~
\begin{aligned}
    L(\theta) \approx& 类似于泰勒的二阶展开\\
    & L(\theta') + {\color{red}(\theta-\theta')^Tg} + {\color{orange}\frac12(\theta-\theta')^TH(\theta-\theta')}\\
    =& L(\theta') + {\color{red}0} + {\color{orange}\frac12 v^THv}，梯度为0\\~\\
    
    & 梯度g是vector\\
    g=& \bigtriangledown L(\theta')，g_i= \frac{\partial L(\theta')}{\partial \theta_i}\\
    & hessian~h是matrix，该矩阵？也叫黑塞矩阵？\\
	H_{ij}=& \frac{\partial^2}{\partial\theta_i\partial\theta_j}L(\theta')
\end{aligned}
$$
判断：
$$
\begin{cases}
    \forall~{\color{orange}v^THv}>0，&& 则是局部最小值\\
    \forall~{\color{orange}v^THv}<0，&& 则是局部最大值\\
    \exist~{\color{orange}v^THv}>0且\exist~{\color{orange}v^THv}<0，&& 则是鞍点\\
\end{cases}
$$




举例：

![image-20221027135132907](02. 局部最小值和鞍点.assets/image-20221027135132907.png)



即，我们梯度为0后，还可以看h，去下降

H它是一个矩阵,这个矩阵裡面元素就是L的二次微分

**但是当然实际上,在实际的implementation裡面,你几乎不会真的把Hessian算出来**。
这个要是二次微分，要计算这个矩阵的computation,需要的运算量非常非常的大,更遑论你还要把它的eigen value,跟 eigen vector找出来,所以在实作上,你几乎没有看到,有人用这一个方法来逃离saddle point



从经验上看起来,其实local minima并没有那麼常见,多数的时候,你觉得你train到一个地方,你 gradient真的很小,然后所以你的参数不再update了,往往是因為你卡在了一个saddle point















