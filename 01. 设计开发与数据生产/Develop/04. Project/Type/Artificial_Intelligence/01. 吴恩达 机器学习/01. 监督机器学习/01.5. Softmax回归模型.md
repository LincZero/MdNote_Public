# 吴恩达机器学习

# 目录

# Softmax - 机器学习笔记

## 概念

- 二元分类（Binary Classification）：只有两个分类
  - 例如判断是或不是的情况

- 多分类（Multiclass classification）：有多个分类
  - 例如手写体的数字识别（选择0~9）
  - 例如在一个图像中判断哪些是人哪些是小车哪些是汽车


## ① 定义模型、激活函数

### 二元分类逻辑回归

公式：
$$
\begin{align}
a_1=&g(z)=\frac 1{1+e^{-z}}&&=P(y=1|\vec x)\\
a_2=&1-a_1&&=P(y=0|\vec x)
\end{align}
$$

### 多分类逻辑回归（Softmax）

（Softmax 翻译 分类器，不一定正确）

通用公式：
$$
\begin{align}
z_j=&\vec w_j\cdot\vec x+b_j，~~~j=1,\cdots,N\\
a_j=&\frac{e^{z_j}}{\sum_{k=1}^{N}e^{z_k}}=P(y=j|\vec x)
\end{align}
$$
举例 - 设有四种分类：
$$
先定义四个逻辑回归公式：\\
z_1=\vec w_1\cdot \vec x+b_1\\
z_2=\vec w_2\cdot \vec x+b_2\\
z_3=\vec w_3\cdot \vec x+b_3\\
z_4=\vec w_4\cdot \vec x+b_4\\~\\

然后可求得四种概率分别：\\
\begin{align}
a_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}=P(y=1|\vec x)\\
a_2=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}=P(y=2|\vec x)\\
a_3=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}=P(y=3|\vec x)\\
a_4=\frac{e^{z_4}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}=P(y=4|\vec x)\\
\end{align}\\
相加为1
$$

## ② Loss函数

### 二元分类逻辑回归

$y=0/1$

Loss函数：

$$
Loss=-y\log a_1-(1-y)\log(1-a_1)
$$

### 多分类逻辑回归（Softmax）

由于$y>0$

Loss函数：
$$
Loss(a_1,\cdots,a_N,y)=\
\begin{cases}
	-\log a_1 &&\text{if } y=1\\
	-\log a_2 &&\text{if } y=2\\
	~~~~\vdots && ~~~~\vdots\\
	-\log a_N &&\text{if } y=N\\
\end{cases}
$$

## ③ 神经网络的Softmax输出

### 区分：多类分类 & 多标签分类

先区分以下两个概念，不要混淆

- 多类分类（Multiiclass Classification）（Softmax输出？）
  - 像是之前的例子，例如手写体的数字识别（选择0~9），输出y
  - 输出：$\vec y=[P_1,P_2,\cdots,P_n]，\sum P=1$
- 多标签分类（Multi-label Classification）
  - 我们需要同时检测多个物品的多个类别，例如在一个图像中判断哪些是人哪些是小车哪些是巴士
    繁琐的做法是建立三个神经网络，分别是找小车、巴士、人。但一般来说，更好的做法是只训练一个模型，就能去分辨这三种东西
  - 输出：$\vec y=[P_1,P_2,\cdots,P_n]，\forall P\sub[0,1]$

### Softmax输出层

之前的神经网络的输出层都是只有一个单元，而多分类神经网络的输出层是Softmax输出，Softmax层也称Softmax激活函数，最后的输出是一个向量。

除了这个区别以外，其他的和之前的神经网络训练基本相同

### 一般写法

```python
"""
注意：代码仅供演示，不要抄，后面会学更好的处理方式
"""
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# 1. 定义模型、定义层与激活函数
model = Sequential([		# 比如我们的两个隐藏层使用ReLU，最后的输出层使用Softmax
    Dense(units=25, activation='relu'),
    Dense(units=25, activation='relu'),
    Dense(units=10, activation='softmax'),
])

# 2. 定义Loss和Cost
from tensorflow.keras.losses import SparseCategoricalCrossentropy
model.compile(loss = SparseCategoricalCrossentropy())

# 3. 最优化、模型训练
model.fit(X,Y,epochs=100)

```



# Softmax - 深度学习笔记

## Softmax 回归（Softmax regression）

### 介绍

==Softmax回归，也叫多分类模型。在该路径中还有部分相关的笔记：`../02. 高级学习算法/02. 多分类模型（Softmax）.md`
不过这部分笔记被我复制了一遍到上去了==



到目前为止，我们讲到过的分类的例子都使用了二分分类，那如果我们有多种可能的类型的话呢？

有一种**logistic回归**的一般形式，叫做**Softmax回归**，能让你在试图识别多种分类中的某一分类时做出预测



比如：

你不单需要识别猫，而是想识别猫，狗和小鸡。
我把猫加做类1，狗为类2，小鸡是类3。
如果不属于以上任何一类，就分到“其它”或者说“以上均不符合”这一类，我把它叫做类0。
如图

![](01.5. Softmax回归模型.assets/e36d502aa68bf9e1a118f5d13e24b134-1665515349614198.png)



符号：

大写的$C$来表示你的输入会被分入的类别总个数。指示类别的数字，就是从0到$C-1$
在上面的例子中，我们有4种可能的类别，包括“其它”或“以上均不符合”这一类，类别序号为0、1、2、3。



神经网络的输出：

在这个例子中，我们将建立一个神经网络，其输出层有4个，或者说$C$个输出单元。
我们想要输出层单元的数字告诉我们这4种类型中每个的概率有多大。在输入$X$的情况下，这四个节点分别输出 其他、狗、猫、小鸡 的概率
输出$\hat y$是一个$4×1$维向量，即输出四种概率。它们加起来应该等于1。



### 实现

神经网络最后一层（Softmax层）中，

- 前面的计算和之前一样算出$z^{[l]}$：$z^{[l]} = W^{[l]}a^{[L-1]} + b^{[l]}$。然后你需要应用**Softmax激活函数**。
- 首先，我们要计算一个临时变量$t$：$t=e^{z^{[l]}}$
- 然后，输出$a^{[l]}$：$a^{[l]}=向量t的归一化$

- 其中，该例子中，其中$z^{[l]}$、$t$、$a^{[l]}$尺寸都是4×1

代入数值来捋一遍流程：
$$
① ~算出z^{[l]}\\
z^{[l]} = \begin{bmatrix} 5 \\ 2 \\  - 1 \\ 3 \\ \end{bmatrix}\\~\\


②~计算t\\
t =e^{z^{[l]}}
=\begin{bmatrix} e^{5} \\ e^{2} \\ e^{- 1} \\ e^{3} \\ \end{bmatrix}
= \begin{bmatrix} 148.4 \\ 7.4 \\ 0.4 \\ 20.1 \\ \end{bmatrix}\\~\\


③~归一化，从向量t得到向量a^{[l]}，使概率总和为1\\
a^{[l]} = \frac{e^{z^{[l]}}}{\sum_{j =1}^{4}t_{i}} 
= \frac{t} {\sum_{j =1}^{4}t_{i}}
= \frac{t} {176.3}
=\hat y=\begin{bmatrix} 0.842 \\ 0.042 \\ 0.002 \\ 0.114 \\ \end{bmatrix}\\~\\

④~每一项的概率：\\
a_{i}^{[l]} = \frac{t_{i}}{\sum_{j =1}^{4}t_{i}}\\
类0\sim类3的概率：\\
\begin{cases}
    a_{1}^{[l]}=\frac{e^{5}}{176.3} =0.842\\
    a_{2}^{[l]}=\frac{e^{2}}{176.3} =0.042\\
    a_{3}^{[l]}=\frac{e^{- 1}}{176.3} =0.002\\
    a_{4}^{[l]}=\frac{e^{3}}{176.3} =0.114
\end{cases}\\
$$


一些补充：

之前，我们的激活函数都是接受单行数值输入。
相比**Sigmoid**和**ReLu**激活函数，输入一个实数，输出一个实数。
**Softmax**激活函数的特殊之处在于需要将所有可能的输出归一化，需要输入一个向量，输出一个向量。



### 那么Softmax分类器还可以代表其它的什么东西么？

> 比如你有两个输入$x_{1}$，$x_{2}$，它们直接输入到**Softmax**层，它可以有三四个或者更多的输出节点
>
> 
>
> 比如输出有三个节点的例子（$C=3$）
>
> ![](01.5. Softmax回归模型.assets/1dc4bf21b6e38cbe9bdfb729ed969c99-1665519351471201.png)
>
> 原始输入只有$x_{1}$和$x_{2}$，一个个输出分类的Softmax层能够代表这种类型的决策边界
>
> 请注意这是几条**线性决策边界**，但这使得它能够将数据分到3个类别中
>
> 图中的颜色显示了Softmax分类器的输出的阈值，输入的**着色是基于三种输出中概率最高的那种**
>
> 
>
> 比如有更多分类项的例子（$C依次=4,5,6$）
>
> ![](01.5. Softmax回归模型.assets/7207527be03bc1daec77afb6c29b8533-1665519384498203.png)
>
> 基本同理
>



## 训练一个 Softmax 分类器（Training a Softmax classifier）

你将更深入地了解**Softmax**分类，并学习如何训练一个使用了**Softmax**层的模型。

复习一下前面的例子
$$
z^{[L]}=\begin{bmatrix} 5 \\ 2 \\  - 1 \\ 3 \\ \end{bmatrix}，
t=\begin{bmatrix} e^{5} \\ e^{2} \\ e^{- 1} \\ e^{3} \\ \end{bmatrix}\\

a^{[L]}=g^{[L]}(z^{[L]})=
\begin{bmatrix}
	e^{5}/(e^5+e^2+e^{-1}+e^3) \\
	e^{2}/(e^5+e^2+e^{-1}+e^3) \\
	e^{-1}/(e^5+e^2+e^{-1}+e^3) \\
	e^{3}/(e^5+e^2+e^{-1}+e^3) \\
\end{bmatrix}
=
\begin{bmatrix}	0.842\\0.042\\0.002\\0.114\end{bmatrix}
$$



### Softmax的名字来源

> **Softmax**这个名称的来源是与所谓**hardmax**对比。我不知道这是不是一个好名字，但至少这就是softmax这一名称背后所包含的想法，与hardmax正好相反。
>
> hardmax：
>
> hardmax函数会观察$z$的元素，然后在$z$​中最大元素的位置放上1，其它位置放上0。即最大的元素的输出为1，其它的输出都为0。例如：
> $$
> z=\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \\ \end{bmatrix}
> $$
> Softmax：
>
> 与之相反，所做的从$z$​到这些概率的映射更为温和
> $$
> z=\begin{bmatrix} P_0 \\ P_1 \\ P_2 \\ P_3 \\ \end{bmatrix}
> $$



### $C=2$的特殊情况

> **Softmax**回归或**Softmax**激活函数将**logistic**激活函数推广到$C$类，而不仅仅是两类，结果就是如果$C=2$，那么$C=2$​的**Softmax**实际上变回了**logistic**回归。
>
> 不严格证明：
> $$
> Logistic回归：\\
> g(z)=\frac{1}{1+e^{-z}}，~g(z)\sub (0,1)\\~\\
> 
> Softmax回归：\\
> (C=2时)\\
> a^{[L]}=g^{[L]}(z^{[L]})=
> \begin{bmatrix}
> 	e^{z_1}/(e^{z_1}+e^{z_2}) \\
> 	e^{z_2}/(e^{z_1}+e^{z_2}) \\
> \end{bmatrix}
> =
> \begin{bmatrix}
> 	e^{z_1}\frac{1}{(1+e^{z_2-z_1})} \\
> 	1-a^{[L]}_1 \\
> \end{bmatrix}
> $$



### 训练

训练Softmax输出层的神经网络

#### 损失函数

损失
$$
训练集中某个样本的目标输出：\\
y_真=\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ \end{bmatrix}\\
a^{[l]}=\hat y=\begin{bmatrix} 0.3 \\ 0.2 \\ 0.1 \\ 0.4 \\ \end{bmatrix}\\
这实际上是一只猫，但却只分配到20\%是猫的概率，所以在本例中表现不佳
$$
损失函数
$$
\begin{aligned}
	\text{Logistic回归损失函数}：& Loss=-y\log \hat y-(1-y)\log(1-\hat y) &&(y=0,1)\\
    \text{Softmax损失函数}：& L(\hat y,y ) = - \sum_{j = 1}^{4}{y_{j}\log\hat y_{j}} &&(y>0)
\end{aligned}
$$
例如该例子中
$$
\begin{aligned}    
    \because~& y_{1} =y_{3} = y_{4} = 0，y_{2} =1\\
    \therefore~& L\left( \hat y,y \right)
    = -\sum_{j = 1}^{4}{y_{j}\log \hat y_{j}}
    = -y_{2}\log \hat y_{2}
    = -\log \hat y_{2}
\end{aligned}
$$
一些变量的尺寸
$$
Y=\lbrack y^{(1)}\text{}y^{(2)}\ldots\ldots\ y^{\left( m \right)}\rbrack
=\begin{bmatrix} 0 & 0 & 1 & \ldots \\ 1 & 0 & 0 & \ldots \\ 0 & 1 & 0 & \ldots \\ 0 & 0 & 0 & \ldots \\ \end{bmatrix}\\

\hat{Y} = \lbrack{\hat{y}}^{(1)}{\hat{y}}^{(2)} \ldots \ldots\ {\hat{y}}^{(m)}\rbrack
= \begin{bmatrix} 0.3 & \ldots \\ 0.2 & \ldots \\ 0.1 & \ldots \\ 0.4 & \ldots \\ \end{bmatrix}\\

Y和\hat Y都是一个4×m维矩阵
$$


#### 代价函数

$$
J( w^{[1]},b^{[1]},\ldots\ldots) = \frac{1}{m}\sum_{i = 1}^{m}{L( \hat y^{(i)},y^{(i)})}
$$



#### 最优化

最后我们来看一下，在有Softmax输出层时如何实现梯度下降法
$$
dz^{[l]} = \frac{\partial J}{\partial z^{[l]}}= \hat{y} -y\\~\\
你可以用~\hat{y}~这个C×1向量减去~y~这个C×1向量
$$









