# 吴恩达机器学习

# 目录

# AI

## AI、ML、DL 关系

- 包含关系：$深度学习\sub 机器学习\sub 人工智能$
  - 人工智能（Artificial Intelligence，AI）
  - 机器学习（Machine Learning，ML）
  - 深度学习（Deep Learning，DL）

## AI，人工智能（Artificial Intelligence）

分类：

- **通用人工智能 / 强人工智能**（**AGI**，Artificial General Intelligence），字面翻译是“通用”，“强”可能是意译吧
  - 可以做任何事。有很多关于AGI的不必要的宣传，现在的AGI还是不够厉害
- **狭义人工智能**（**ANI**，Artificial Narrow Intelligence），与AGI相对应
  - 只能做一件事，用于特定场所。例如下棋、自动驾驶

## ML，机器学习（Machine Learning）

### ML：定义

现有两种机器学习的定义

- 定义一
  - 亚瑟·塞缪尔（Arthur Samuel）定义：
    “使计算机无需进行明确编程即可习得的研究领域。”
  - 这是一个较旧的非正式定义。
- 定义二
  - 汤姆·米切尔（Tom Mitchell）定义：
    “如果某计算机程序在 任务T (Task) 上的 性能P (Performance) 随着 经验E (Experience) 的增加而提高，
    那么可以说计算机程序可以从 经验E 中学习一些 任务T 和 [性能指标](https://so.csdn.net/so/search?q=性能指标&spm=1001.2101.3001.7020)P。 ” 
  - 这是一个更现代的定义
  - *例如：玩跳棋。
    E ：玩过许多跳棋游戏的经验。
    T ：扮演跳棋的任务。
    P ：程序赢得下一场比赛的概率。*

### ML：分类

*除视频外可参考：*

- [【Quora问答】监督学习算法和无监督学习算法的区别](https://www.quora.com/What-is-the-difference-between-supervised-and-unsupervised-learning-algorithms?)
- [【知乎】机器学习算法之有监督学习和无监督学习的区别](https://zhuanlan.zhihu.com/p/118486542)
- [【CSDN】监督学习和无监督学习区别](https://blog.csdn.net/zuochang_liu/article/details/90297321)



通常，可以将任一机器学习问题分配给以下两种广泛的分类之一：

- **监督学习**（supervised learning）
  - 特点：有训练样本，需要大量被标记的数据
  - 应用分类：监督学习解决两种类型的问题：
    - 回归（regression），例如 预测播放量，$L(f,(X,Y))=||f(X)-Y||^2$
      - 有很多种类的回归模型：线性回归、多项式回归、逻辑回归（不过逻辑回归一般用来作分类而不是预测）
    - 分类（classification），例如 识别猫还是狗，$L(f,(X,Y))=-\log f_Y(X)$，其中$f_Y(X)\geq0，\sum_i f_i(X)=1$
      - 二元分类（Binary Classification）：只有两个分类，例如判断是或不是的情况
      - 多分类（Multiclass classification）：有多个分类，例如手写体的数字识别（选择0~9）

  - 算法：一些常见的监督学习算法包括
    - 线性和逻辑回归
    - 朴素贝叶斯
    - 支持向量机
    - 决策树和随机森林
    - 人工神经网络

- **无监督学习**（unsupervised learning）
  - 特点：无训练样本，不需要标记数据
  - 应用：集群问题（最主要用途）、降维、安全分析中的异常检测
  - 算法：
    - K-means（用于聚类）
    - 主成分分析（PCA）（用于降维）
    - ……



#### 监督学习

在**监督学习**中，我们得到一个数据集，并且已知**正确的输出**应该是什么样的，以及认为**输入与输出有关联**。

监督学习问题分为“回归”和“分类”问题

- 在**回归**问题中，我们试图预测**连续**输出中的结果，这意味着我们试图将输入变量映射到某个连续函数。
- 在**分类**问题中，我们尝试预测**离散**输出中的结果，这意味着我们正在尝试将输入变量映射为离散类别。



例如：

> 电子邮件判断是否垃圾、音频转文字、语言机器翻译、用人的信息和广告来判断是否会点击、判断图片里的是什么物体、产品质量合格检测等等

范例1：

> 给定有关房地产市场上房屋大小的数据，请尝试预测其价格。价格作为规模的函数是一个连续的输出，因此这是一个**回归**问题。
>
> ![回归问题](00. AI.assets/回归问题.png)
>
> 我们可以通过输出 “房屋是否以高于或低于预测价格出售”，从而将这个例子变成**分类**问题，根据价格将房屋分为两类。

范例2：

> （a）**回归**——给定男性/女性的照片，根据给定的照片预测他/她的年龄。
>
> （b）**分类**——给定一张男性/女性的照片，预测他/她的年龄是属于高中、大学还是研究生。
> 分类的另一个示例——银行必须根据某人的信用记录来决定是否向某人提供贷款。

#### 无监督学习

另一方面，**无监督学习**使我们几乎或根本**不了解输出结果**应该是什么样子。我们可以从数据中获得结构，而不必知道变量的影响。

我们可以通过基于数据中变量之间的关系对数据进行**聚类**来推导此结构。

在无监督学习的情况下，没有基于预测结果的反馈，即**无法纠正**。



范例：

> **聚类**——收集有关《美国经济》的1000篇文章，并找到一种方法，将这些**文章自动归类**，
> 这些变量在某种程度上因不同的变量而相似或相关，例如词频、句子长度、页数。
>
> ![文章自动归类](00. AI.assets/文章自动归类.png)

范例2

> **非集群**——**“鸡尾酒会算法”**，可以在混乱的数据中找到结构。例如，在鸡尾酒会上从声音网格中识别单个声音和音乐。
>
> ![非集群——鸡尾酒会算法1](00. AI.assets/非集群——鸡尾酒会算法1.png)
>
> ![非集群——鸡尾酒会算法2](00. AI.assets/非集群——鸡尾酒会算法2.png)



[维基百科:鸡尾酒会算法](https://en.wikipedia.org/wiki/Cocktail_party_effect)
[Octave实现鸡尾酒会算法代码_(SVD)奇异值分解](https://stackoverflow.com/questions/20414667/cocktail-party-algorithm-svd-implementation-in-one-line-of-code)
[matlab实现鸡尾酒会算法代码](https://cs.nyu.edu/~roweis/kica.html)













